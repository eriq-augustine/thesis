% Eriq Augustine
%
% Cal Poly Thesis
%
% based on UC Thesis format
%
% modified by Mark Barry 2/07.
%

\documentclass[12pt]{ucthesis}

\usepackage{etex}
\usepackage[morefloats=125]{morefloats}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[letterpaper]{geometry}
\usepackage[overload]{textcase}
\usepackage{color}
\usepackage[nonumberlist,toc]{glossaries}
\usepackage{wrapfig}
\usepackage{longtable}
\usepackage{morefloats}
\usepackage{float}
\usepackage{listings}
\usepackage{makecell}

\definecolor{orange}{rgb}{1,0.5,0}
\definecolor{blue}{rgb}{0.1,0.1,0.8}
\definecolor{green}{rgb}{0,0.5,0}
\definecolor{red}{rgb}{0.8,0,0}
\definecolor{bad}{rgb}{0.8,0,0.8}

\makeindex
\makeglossaries

\bibliographystyle{abbrv}

\setlength{\parindent}{0.25in} \setlength{\parskip}{6pt}
\geometry{verbose,nohead,tmargin=1.25in,bmargin=1in,lmargin=1.5in,rmargin=1.3in}
\setcounter{tocdepth}{2}

% Different font in captions (single-spaced, bold) ------------
\newcommand{\captionfonts}{\small\bf\ssp}

\makeatletter  % Allow the use of @ in command names
\long\def\@makecaption#1#2{%
  \vskip\abovecaptionskip
  \sbox\@tempboxa{{\captionfonts #1: #2}}%
  \ifdim \wd\@tempboxa >\hsize
    {\captionfonts #1: #2\par}
  \else
    \hbox to\hsize{\hfil\box\@tempboxa\hfil}%
  \fi
  \vskip\belowcaptionskip}
\makeatother   % Cancel the effect of \makeatletter
% ---------------------------------------

\begin{document}

% Declarations for Front Matter

% Update fields below!
\title{SPOONS: Netflix Outage Detection Using Microtext Classification}
\author{Eriq Augustine}
\degreemonth{March} \degreeyear{2012} \degree{Master of Science}
\defensemonth{March} \defenseyear{2012}
\numberofmembers{3} \chair{Alex Dekhtyar, Ph.D.} \othermemberA{Clint Staley, Ph.D.} \othermemberB{Franz Kurfess, Ph.D.} \othermemberC{Foaad Khosmood, Ph.D.} \field{Computer Science} \campus{San Luis Obispo}
\copyrightyears{seven}

\maketitle

\begin{frontmatter}

% Custom made for Cal Poly (by Mark Barry, modified by Andrew Tsui).
\copyrightpage

% Custom made for Cal Poly (by Andrew Tsui).
\committeemembershippage

\begin{abstract}

Every week there are over a billion new posts to Twitter services and many of
those messages contain feedback to companies about their services. One company
that has recognizes this unused source of information is Netflix. That is why
Netflix initiated the development of a system that lets them respond to the
millions of Twitter and Netflix users that are acting as sensors and reporting all types of user
visible outages. This system enhances the feedback loop between Netflix and
its customers by increasing the amount of customer feedback that Netflix receives
and reducing the time it takes for Netflix to receive the reports and respond to them.

The goal of the SPOONS (Swift Perceptions of Online Negative Situations) system
is to use Twitter posts to determine when Netflix users are reporting a problem
with any of the Netflix services. This work covers the architecture SPOONS system and framework
as well as outage detection using tweet classification.

\end{abstract}

\begin{acknowledgements}

% TODO(eriq): More Thankful!
Thanks Alex, ABRA, Netflix especially Kevin McEntee, and all the funions.
Thanks to Farscape for encouraging team bonding and providing a common enemy.

\end{acknowledgements}

\tableofcontents

\listoftables

\listoffigures

\end{frontmatter}

\pagestyle{plain}

\renewcommand{\baselinestretch}{1.66}

\part{Introduction}
\label{introduction}

\chapter{Problem: Swift Perception Of Online Negative Situations}
\label{general-problem}

Twitter is an immensely popular micro-blogging service. According to Twitter, as of March 14\textsuperscript{th} 2011,
approximately one billion micro-posts, \emph{tweets}, were being posted per week\cite{TwitterBlog}.
Because of the low time and effort cost of tweeting, only a few seconds from a smart phone,
Twitter users post tweets about almost every aspect of their daily lives.
Because of this large stream of information, Twitter makes an excellent source of information for
data miners interested in real-time events. Already, researchers have been using Twitter to attempt to track and model
disease outbreaks\cite{DetectingInfluenza}, earthquakes\cite{Earthquakes}, and the
stock market\cite{StockMarket}.

Netflix is the one of the largest online Internet subscription service for streaming movies and
television shows. Netflix has over 25 million subscribers watching media streamed to over 450 different
platforms. Even a short disruption of their streaming service can affect millions of users. Therefore, quickly detecting service outages
is essential to keep customers happy. However, service outage detection is no trivial matter in Netflix's
environment. In addition to constantly streaming thousands of different videos to hundreds of different platforms,
Netflix also has to deal with problems caused by most of their infrastructure being hosted in the cloud with
Amazon Web Services (AWS).

Netflix saw the power in Twitter as a potential data source for detecting service outages that
is orthogonal to their current, more traditional outage detection methods. Currently, Netflix utilizes four
different methods for detecting outages:

\paragraph{Internal Monitoring Systems.}
Like any sizable service providing company, Netflix utilizes many different internal monitoring
systems to detect service outages. However, there are some classes of problems that are difficult to solve
with internal monitoring. These problems include corrupt video files or a problem on a third-party delivery
platform such as Roku or AppleTV. These problems are obvious to the end user, but very difficult to detect internally.
In addition, the internal monitoring systems share the same infrastructure as the service providing system. Therefore,
a problem in the infrastructure can cause both systems to go down at the same time.

\paragraph{External Monitoring Systems.}
Netflix contracts with external services that can periodically probe its systems to try and detect problems.
However, this model too has problems. There are many problems that cannot be seen from an external probe.
Also, if this system probes too often then it is taking compute time away from the servers that are trying to deliver
content to end users.

\paragraph{Customer Service.}
Calls to customer service are a very straight-forward way to detect outages.
Unfortunately, this method is very slow and inconsistent. It takes a lot of frustration to get a user to
lookup a phone number and complain.

\paragraph{Manual Twitter Observation.}
Manual observation shows that there is usually a response on Twitter when Netflix suffers a service
outage. Figure~\ref{fig:tweetEx} shows some tweets that occurred during a disruption of Netflix's service to
the Nintendo Wii. However without any infrastructure, Twitter observation is slow and inconsistent.
It is also very time consuming to have someone constantly watching Twitter for signs of an outage.

\emph{Given all these deficiencies Netflix wanted a monitoring system that is separate from their infrastructure,
fast, and does not require any human intervention\cite{kevin}.}

\begin{figure}
   \begin{center}
      \includegraphics[width=140mm]{images/tweetexample.eps}
      \captionfonts
      \caption[Outage Tweets Example]{Tweets posted on March 9, 2011 during a disruption of Netflix
                                       streaming to the Nintendo Wii console.}
      \label{fig:tweetEx}
   \end{center}
\end{figure}


\chapter{Solution Overview}
\label{overview}

SPOONS (Swift Perception Of Online Negative Situations) is a system that is
designed to use tweets to detect outages in Netflix content delivery systems.
At present, the system supports a wide variety of detection methods that use some combination of time
series analysis, classification, natural language processing, sentiment
analysis, and filtering.

\begin{figure}
   \begin{center}
      \includegraphics[width=140mm]{images/systemFlow.eps}
      \captionfonts
      \caption[System Concept Diagram]{This system concept diagram shows the general
                                       flow of processing done in the SPOONS system.}
      \label{fig:systemFlow}
   \end{center}
\end{figure}

Figure~\ref{fig:systemFlow} shows how the SPOONS system can be divided into three
main parts: input; analysis pipelines; and output.
The inputs are tweets gathered from Twitter. Then the
analysis pipelines use a combination of sentiment estimation, classification, and
traffic volume analysis to detect when an outage is occurring.
The outputs of the system are: email alerts to Netflix engineers, and a web UI that displays
information about the outage.

\chapter{Ethics of Twitter Observation}

The work in this project uses content that users post on Twitter without their
knowledge. This monitoring system isn't being announced to the
public because widespread knowledge of it would increase the likelihood of a
malicious attack. This practice may lead to concerns about the level of privacy
or ownership being provided to Twitter users regarding the content they post
through the Twitter services. The goal of this section is to address these
concerns by providing more information about the Twitter services and how the
SPOONS system and this work uses the tweets.

\section{Twitter Terms of Service}

According to Twitter Terms of Service\cite{termsOfService} agreement that
everyone accepts automatically by accessing or using Twitter services:

\emph{``You retain your rights to any Content you submit, post or
display on or through the Services. By submitting, posting or displaying Content
on or through the Services, you grant us a worldwide, non-exclusive,
royalty-free license (with the right to sublicense) to use, copy, reproduce,
process, adapt, modify, publish, transmit, display and distribute such Content
in any and all media or distribution methods (now known or later developed).''}

\emph{``This license is you authorizing us to make your Tweets available to the
rest of the world and to let others do the same.''}

\emph{``You agree that this license includes the right for Twitter to make such
Content available to other companies, organizations or individuals who partner with
Twitter for the syndication, broadcast, distribution or publication of such
Content on other media and services, subject to our terms and conditions for
such Content use.''}

\emph{``We encourage and permit broad reuse of Content. The Twitter API exists to
enable this.''}

\emph{``Such additional uses by Twitter, or other companies, organizations or
individuals who partner with Twitter, may be made with no compensation paid to
you with respect to the Content that you submit, post, transmit or otherwise
make available through the Services.''}

In short, Twitter takes ownership of user tweets as soon as they are posted on Twitter.
Using the Twitter API allows SPOONS to obtain the tweets with the consent of Twitter.
Therefore, the collection and analysis of Twitter data by SPOONS is well withing the
Twitter Terms of Service.

\chapter{SPOONS Requirements}
Netflix has provided the following set of key requirements to be met by the
SPOONS system:

\paragraph{Structural Independence.}
The outage detection system shall be structurally independent of both the
software and the hardware infrastructure used by Netflix. It shall rely only on
information that is publicly available and free for use. This ensures that the
outage detection system stays up even when any or all Netflix servers are
experiencing downtime.

\paragraph{Use of Amazon Web Services.}
Netflix is one of the largest customers of Amazon.com's cloud computing
service, Amazon Web Services (AWS). AWS allows users to create new cloud
machines (instances) in many regions throughout the world. The outage
detection system shall be deployed on one or more AWS servers that are
operationally independent of other AWS servers used by Netflix. Using a cloud
solution allows the outage detection and alert system to be deployable on a
global scale.

\paragraph{Real-Time.}
Netflix's streaming services run in real-time and any downtime has an immediate
impact on customers. To minimize that impact, the outage detection system shall notify
Netflix of detected outages as soon as possible.

\paragraph{Precise Outage Detection.}
The number of non-outage situations that raise an alert shall be minimized.
While a small number of false positives detected in real-time may be acceptable,
the outage detection system shall detect outages and generate alerts with as
high precision as possible.

\paragraph{Comprehensive Outage Detection.}
Not all Netflix service outages generate a signal on Twitter. Those that don't may
be allowed to go unnoticed by the outage detection system (as the system
has no basis for detecting them), but any outage that causes a signal on
Twitter shall be detected.

\paragraph{User-Friendly Online UI.}
The outage detection and alert system shall have an easy-to-use, informative,
online UI which shall provide Netflix employees with real-time information and
historic data about the state of Netflix according to Twitter. The information
provided shall include:

\begin{itemize}
   \item times of outages;
   \item times of other anomalous events;
   \item current and recent Netflix-related Twitter traffic trends;
   \item and samples of Netflix-related tweets.
\end{itemize}

\chapter{Contributions and Organization}
\label{contributions-organization}

SPOONS is a continual team effort and has been touched and improved by many different people.
The idea originated at Netflix and was passed to the ABRA team at Cal Poly.
The ABRA team has published a paper on SPOONS \cite{abraPaper}.
In addition, Cailin Cushing defended a thesis on a part of SPOONS devoted to outage detection through sentiment analysis\cite{cailinThesis}.

The main contributions of this work are as follows:

\begin{itemize}
   \item Design and implementation of the SPOONS system.
   \item Design and implementation of the SPOONS framework.
   \item Design and implementation of the SPOONS server architecture.
   \item Design and implementation of the SPOONS distributed computation model.
   \item Design of the SPOONS database structure and all table schemas.
   \item Design and implementation of all SPOONS classification based outage detection methods.
\end{itemize}

The rest of the paper is organized as follows.
Chapter~\ref{background-related-work} covers background and related work.
Part~\ref{arch} discusses the architecture of SPOONS.
Part~\ref{analysis} discuss the work done by the spoons system to detect outages.
With Chapter~\ref{classifiers} focusing on the details of the classifiers used in SPOONS, and
Chapter~\ref{outage-detection} extending the problem of classification to full outage detection.
Part~\ref{conclusions} wraps up the paper.

\chapter{Background \& Related Work}
\label{background-related-work}

%\chapter{Text Stream Analysis}
%\label{background-text-stream}
%Text Stream Analysis \cite{Bansal}\cite{Grinev}\cite{Huang}.

\section{Twitter Traffic Analysis}
\label{background-twitter}
Twitter proves to be a great resource for data mining because of the large number of real-time, posts from millions of users.
However, tweets can be very difficult to work with because they suffer from three large drawbacks:

\paragraph{Length.}
Tweets can only be 140 characters long. This limit severely restricts the possible information content of a tweet.
Compared to more traditional media sources, e.g. news articles, the text of tweets contain almost no information.
Although this makes it very difficult to do naive text classification on tweets, Twitter users have found ways to increase their information density.
Links to news stories, slang, and Twitter symbols (see Section~\ref{class-filter-twitter-symbols}) help Twitter users express more with fewer characters.

\paragraph{Informal Language.}
Informal language, e.g. slang, jargon, and abbreviations, is common place on Twitter.
The use of informal language can be partially attributed to the strict character limit.
Informal language can be difficult to deal with because it it less likely to appear in well established
Natural Language Processing corpora. Not having a corpus forces researchers to either only use unsupervised methods, or
build their own corpus.

\paragraph{Typos.}
The nature of Twitter is very informal for most users whose tweets are only read by their friends.
This informal environment and the large amount of tweets coming from hand-held devices without a traditional keyboard leads to many tweets containing typos.
Typos make text analysis difficult because it obfuscates words and increases the number of unique words.

\subsection{Twitter Classification}
\label{background-twitter-classification}
There has been much work in using classifiers on both tweets and Twitter users.
Most of the classification efforts has gone into trying to determine the sentiment,
the general feeling, of a tweet \cite{Jiang}\cite{Mukherjee}\cite{Saif}\cite{Wang}.
Raz et al. tackle the task of classifying humorous tweets as a specific type of humor
such as irony, observational, or wordplay\cite{Raz}.
The traditional text classification task of topic modeling has also been attempted various times\cite{hong}\cite{Zhao}.
Instead of trying to classify tweets, Pennacchiotti et al. try to classify user associations from
their tweets\cite{Pennacchiotti}.

\subsection{Twitter Anomaly Detection}
\label{background-twitter-anomaly}
Levchenko et al.\cite{levchenko} created a system that uses tweets to detect outages in several widely used Web services such as Amazon, Gmail, Google, PayPal, Netflix, Youtube, Facebook, Wikipedia, and Flickr.
They describe Twitter users as acting as millions of sensors who have a large breadth and flexibility of in the definition of failure.
The detection mechanism employed in this work is fairly straightforward.
A collection of tweets that either contain the phrase ``\texttt{X is down}'' or a ``\texttt{\#Xfail}'' hashtag, where ``X'' is the name of a service (e.g., ``\texttt{\#netflixfail}'') is gathered.
The traffic is compared against expected traffic to determin if there is an outage.

Levchenko et al. were only able to validate a subset of their detected events because a full validation would require a list of all outages during 2009 for every service that they were monitoring.
So while the events they were able to verify indicate that the system can detect outages, the full effectiveness of their method is still largely unknown.

\section{Classifiers}
\label{background-classifiers}
SPOONS uses a variety of different classifiers for text classification.
This section gives an overview of each different type of classifier used.

\subsubsection{Formal Definition}
\label{background-classifiers-def}
The classification problem that the classifiers are trying to solve can be defined as follows:

Given a set of documents $D$
\begin{equation*}
   D = \left \{ d | d \in D \right \}
\end{equation*}
where each document $d$ is a vector of $n$ features
\begin{equation*}
   d = (\textrm{f}_{1}, \textrm{f}_{2}, ..., \textrm{f}_{n})
\end{equation*}
and a set of classes $C$ where
\begin{equation*}
   C = \left \{ c | c \in C \right \}
\end{equation*}
We want to associate each document with a class based off the patterns observed in a training set $T$ of already classified documents.
\begin{equation*}
   T = \left \{ (d, c) | c \in C \right \}
\end{equation*}

\subsection{Naive Bayes}
\label{background-classifiers-naive-bayes}
Naive Bayes classifier works by applying the Bayes' theorem with the assumption that
the probability of each feature in a document is independent from the probability of
any other feature appearing in the same document.\cite{Kibriya}\cite{Frank}

The Bayes' theorem states that the probability of observing class $c$ given document $d$, $Pr(c|d)$, can be represented as:

\begin{equation}
   Pr(c|d) = \frac{Pr(c) \cdot Pr(d|c)}{Pr(d)}
\end{equation}

$Pr(c)$ is the \textsf{prior probability} of class $c$, that is, the probability of
observing $c$ regardless of the document attached to it. When training the classifier, this
is just the percentage of times that the class appeared in the training set.

$Pr(d)$ is the \textsf{prior probability} of document $d$. Like $Pr(c)$, it is just the
probability of observing the collection of features $d$ regardless of the class associated with it.
Note that for classification, it is not necessary to compute $Pr(d)$ because it is constant among all
documents and classes. A classifier can just choose the class with the largest $Pr(c) \cdot Pr(d|c)$ term.

$Pr(d|c)$ is the probability of observing document $d$ given that $d$ is already recognized as belonging to
class $c$. Remember that document $d$ is really just a vector of $n$ features, $(\textrm{f}_{1}, \textrm{f}_{2}, ..., \textrm{f}_{n})$.
Assuming \textbf{conditional independence} (the \textsf{naive} part in Naive Bayes), $Pr(d|c)$ can be
constructed as a product of the probability of observing each feature in $d$:

\begin{equation}
   Pr(d|c) = Pr(\textrm{f}_{1}|c) \cdot Pr(\textrm{f}_{2}|c) \cdot ... \cdot Pr(\textrm{f}_{n}|c) = \prod_{i = 1}^{n}Pr(\textrm{f}_{i}|c)
\end{equation}

Now the last step is to estimate the conditional probabilities of the $n$ features.
When dealing with discrete features, then estimating $Pr(\textrm{f}_{m}|c)$ ($1 \leq m \leq n$) can be done by
finding the percentage of training documents that contain feature $\textrm{f}_{m}$ and have class $c$.

\subsection{Bayes Net}
\label{background-classifiers-bayes-net}
A Bayesian Network is a probabilistic, directed acyclic graphs that represents a set of random variables and their conditional probabilities.
In a Bayesian Network, the collection of incoming edges represent the conditional probability distribution between two random variables.
Each node represents a variable and a probability function that takes as input the state of the node's parents.\cite{Pearl}\cite{Neapolitan}

Figure~\ref{fig:bayesNet1} shows a simple Bayesian Network that models the chance of going on a picnic.
Note that whether or not it is Spring affects the chance of it raining; and both the season and weather
affect the chance of going on a picnic.

Figure~\ref{fig:bayesNet2} shows the probability distributions for the network. The chance of the season being Spring if
fully independent, and therefore takes no parameters into its probability function. However, the weather and picnic decision
takes one and two input parameters respectively.

\begin{figure}
   \begin{center}
      \includegraphics[width=0.4\textwidth]{images/Bayes_Net_1.eps}
      \captionfonts
      \caption[Simple Bayes Net]{A simple Bayesian Network modeling the chance of going on a picnic given the season and weather. The season affects the weather and both the season and weather affect the chance of going on a picnic.}
      \label{fig:bayesNet1}
   \end{center}
\end{figure}

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/Bayes_Net_2.eps}
      \captionfonts
      \caption[Bayes Net With Probabilities]{The simple Bayesian Network augmented with the probability functions for each node.}
      \label{fig:bayesNet2}
   \end{center}
\end{figure}

\subsection{J48}
\label{background-classifiers-j48}
J48 is a specific implementation of the C4.5 algorithm.
C4.5 is an algorithm that is used to generate a decision tree given a training set.

\subsubsection{Decision Trees}
\label{background-classifiers-j48-decision-trees}
A decision tree is a simple data structure used to come to come conclusion based off of a number of observations.
At each non-terminal node, a question is asked. The answers to the question are represented by the node's outgoing edges.
The tree is traversed in this fashion until a terminal node is reached. The terminal node contains the final conclusion.
In a classification context, each non-terminal node is labeled with an attribute, each edge is the value (or range of values)
for that attribute, and each terminal node is a class. Each attribute can only appear once in the tree.

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/Decision_Tree.eps}
      \captionfonts
      \caption[Simple Decision Tree]{A simple decision tree trying to answer the question of whether or not to go on a picnic.}
      \label{fig:decisionTree}
   \end{center}
\end{figure}

Figure~\ref{fig:decisionTree} shows a decision tree that may be generated for the picnic example discussed in Section~\ref{background-classifiers-bayes-net}.
Note that once the decision tree is built, reaching a terminal node is fairly trivial.

\subsubsection{C4.5 --- Decision Tree Induction Algorithm}
\label{background-classifiers-j48-c45}
C4.5 recursively builds a decision tree by continually splitting the dataset on a single attribute\cite{Quinlan}.
The splitting attribute is determined by the normalized information gain (Kullback-Leibler divergence) and becomes a
node in the tree and the possible values for the attribute become edges. Each subtree is then recursively built using only the
data where the splitting attribute takes the value given by the incoming edge. The algorithm has two stopping conditions.
First, when all the data has the same class. In which case a single node tree is constructed that contains the class.
Secondly, when there are no more attributes or when the information gain from splitting on each attribute is below a threshold.
In this case, a single node tree is constructed which contains the plurality class.

\subsection{K-Nearest Neighbors}
\label{background-classifiers-knn}
$k$-Nearest Neighbors (KNN) is simple and effective classification technique\cite{Duda}.
While training, the classifier remembers the entire training set.
During the classification phase, the classifier finds the $k$ nearest neighbors
to the query point. The predicted class is simply the plurality of the $k$ nearest neighbors.
Figure~\ref{fig:knn} shows an example of $k$-Nearest Neighbors with a simple search space.

\begin{figure}
   \begin{center}
      \includegraphics[width=0.4\textwidth]{images/KNN.eps}
      \captionfonts
      \caption[K-Nearest Neighbors]{A simple example of KNN. If $k = 3$, then the query point (the star) will be classified as a triangle. However, if $k = 5$ then the query point will be classified as a square.}
      \label{fig:knn}
   \end{center}
\end{figure}

\subsection{Support Vector Machines}
\label{background-classifiers-svm}
Support Vector Machines (SVMs) are considered one of the best off-the-shelf classification techniques\cite{Vapnik}.
When training, SVMs use hyperplanes to partition the data into surfaces based off of the different classes of the training examples.
When classifying, the SVM finds which surface the query point falls on and give that class to the point.
SVMs try and choose the partitioning hyperplane to maximize the margin between the two groups of data.
Depending on the implementation, the SVM may choose the optimal partition or just an approximation.

\begin{figure}
   \begin{center}
      \includegraphics[width=0.4\textwidth]{images/SVM.eps}
      \captionfonts
      \caption[Support Vector Machine]{A simple example of a support vector machine. The SVM chose a partition that maximizes the margin between the squares and triangles.}
      \label{fig:svm}
   \end{center}
\end{figure}

Figure~\ref{fig:svm} shows a simple example of a linear binary SVM.
Note that the partition line is chosen to maximize the distance between the triangles and squares.
The query point (the star) falls into the squares' partition and is therefore classified as a square.

\subsubsection{Sequential Minimal Optimization}
\label{background-classifiers-svm-smo}
Sequential Minimal Optimization (SMO) is an efficient algorithm for solving SVMs invented by John Platt in 1998\cite{Platt}.

\subsection{BPNB}
\label{background-classifiers-bpnb}
BPNB is a method developed by Chu\cite{bpnb}.
It is based off of Naive Bayes, except the relative probability of each feature is accounted for.

BPNB states that the probability of observing class $c$ given document $d$, $Pr(c|d)$, can be represented as:

\begin{equation}
   Pr(c|d) = Pr(c) \cdot \prod_{i = 1}^{n}g(\textrm{f}_{i},c)
\end{equation}

Where $g(\textrm{f}_{m}, c)$ is the weight of feature $\textrm{f}_m$ in class $c$.

\begin{equation}
   g(\textrm{f}_m, c) = \beta^{1 - \frac{Pr(\textrm{f}_m|c)}{Ave(\textrm{f}_m)}}, 0 < \beta < 1
\end{equation}

\begin{equation}
   Ave(\textrm{f}_m) = \frac{\sum_{i=1}^{|C|} Pr(\textrm{f}_m|c_{i})}{|C|}, c_i \in C
\end{equation}

\subsection{WEKA}
\label{background-weka}
SPOONS utilizes several classifiers provided in the \textit{WEKA Machine Learning Package}.
WEKA is an open source package written under the GNU General Public License\cite{weka}.

\section{Singletons}
\label{background-singletons}
The SPOONS architecture makes heavy use of singletons to guarantee certain assumptions.
A \textbf{singleton} is an object-oriented class that may have at most one instance of itself instantiated at a time.
SPOONS uses two types of singletons: singletons that are relative to the base class and singletons that are relative to the
child classes.

\paragraph{Base Relative Singletons.}
Base relative singletons are singletons that only allow one instance of to base class in the inheritance hierarchy to be instantiated
at a time. This means that there can only be one instance allowed for the entire inheritance hierarchy.
Figure~\ref{fig:baseSingleton} shows an inheritance diagram of a hierarchy that uses a base relative singleton.
Note that because one of the children have been instantiated, no other class in the hierarchy can be instantiated.

\begin{figure}[H]
   \begin{center}
      \includegraphics[width=0.6\textwidth]{images/Base_Singleton.eps}
      \captionfonts
      \caption[Base Relative Singleton]{An example base relative singleton inheritance hierarchy. Note that instantiating any child removes the ability to instantiate any other part of the hierarchy.}
      \label{fig:baseSingleton}
   \end{center}
\end{figure}

\paragraph{Child Relative Singletons.}
Child relative singletons are singletons that allows only one instance of each leaf child in the inheritance hierarchy to be instantiated
at a time. This allows the inheritance hierarchy to have as many instance as leaf children.
Figure~\ref{fig:childSingleton} shows an inheritance diagram of a hierarchy that uses child relative singletons.
Note that all the children can be instantiated once.

\begin{figure}[H]
   \begin{center}
      \includegraphics[width=0.6\textwidth]{images/Child_Singleton.eps}
      \captionfonts
      \caption[Child Relative Singleton]{An example child relative singleton inheritance hierarchy. Note that any child can be instantiated, but only once.}
      \label{fig:childSingleton}
   \end{center}
\end{figure}

\section{Accuracy Measures}
\label{background-accuracy}
The accuracy of classification is primarily determined using three metrics: Recall, Precision, and F Score.

Consider the situation of trying to classify documents into class $A$ with $A$ and $B$ being the two possible classes.
The following definitions are used to calculate the accuracy metrics:
\begin{itemize}
   \item tp --- True Positive. A document with the true class $A$ was correctly classified as $A$.
   \item fp --- False Positive. A document with the true class $B$ was incorrectly classified as $A$.
   \item fn --- False Negative. A document with the true class $A$ was incorrectly classified as $B$.
   \item tn --- True Negative. A document with the true class $B$ was correctly classified as $B$.
\end{itemize}

\subsection{Recall}
\label{background-accuracy-recall}
The percent of the documents that were correctly classified.
\begin{center}
   $Recall = \frac{tp}{tp + fn}$
\end{center}

\subsection{Precision}
\label{background-accuracy-precision}
The percent of correct classifications of all documents classified as $A$.
\begin{center}
   $Precision = \frac{tp}{tp + fp}$
\end{center}

\subsection{F Score}
\label{background-accuracy-f}
A harmonic mean between recall and precision. The standard F$_{1}$ score evenly weighs precision and recall.
SPOONS uses the F$_{0.5}$ score. F$_{0.5}$ weighs precision more than recall. Precision is being weighed more heavily than recall because
every alert that SPOONS generates would require the intervention of a Netflix engineer. Generating too many
false positives would just cause SPOONS to be ignored.
\begin{center}
   $F_{\beta} = (1 + \beta^{2}) \cdot \frac{precision \cdot recall}{\beta^{2} \cdot precision + recall}$
\end{center}
\begin{center}
   $F_{0.5} = 1.25 \cdot \frac{precision \cdot recall}{.25 \cdot precision + recall}$
\end{center}

\subsection{Coverage}
\label{background-accuracy-coverage}
In the context of outage detection, F$_{0.5}$ score cannot completely capture the effectiveness of an outage detection method.
A flaw in solely relying on the F$_{0.5}$ score is that an outage detection method can produce an unjustly high F score by generating long alerts.
Taken to the extreme, an outage detection method can generate just one alert an have it span the entire evaluation period.
This one alert will capture every service outage, and therefore have a recall of 1.
Also, the single alert it generates will intersect with a real outage which will produce a precision of 1.
No matter the type of F score used, a precision and recall of 1 will result in the highest possible F score of 1.
Figure~\ref{fig:coverage} shows a graphical representation of this problem.

To counteract this, an outage detection's coverage is also taken into account.
Coverage is the percentage of frames in the evaluation period that are during alerts.

\begin{figure}[H]
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/Coverage.eps}
      \captionfonts
      \caption[Coverage Example]{A long alert producing an unjustly high precision and recall.}
      \label{fig:coverage}
   \end{center}
\end{figure}

\chapter{Twitter API}
\label{api}
All of the data data that SPOONS uses is obtained in real time using the Twitter Search REST API\cite{TwitterAPI}.

\section{Rate Limiting}
\label{api-rate-limit}
Twitter imposes a limit on the number of queries to the Search API. Twitter does not publish the official
limit. However, our experiments suggest that SPOONS can query the API for all new Tweets once every two minutes without
suffering from rate limiting.

\section{Pagination}
\label{api-pagination}
Twitter paginates the results from its search API. The maximum results you can get per page is 100, and each
query can return at most 15 pages. Therefore when there are more than 1500 tweets generated per minute,
SPOONS must do multiple search queries.

\section{Query Anatomy}
\label{api-anatomy}
The typical structure of a Twitter API query is shown in Figure~\ref{fig:apiQuery}.

\begin{figure}[H]
   \begin{center}
      \fbox{
         \begin{minipage}{12cm}
            \begin{center}
               http://search.twitter.com/search.\textbf{json}?\textbf{q}=$\langle$query$\rangle$\&\textbf{rpp}=100\&\\
               \textbf{result\_type}=recent\&\textbf{since\_id}=$\langle$tweet id$\rangle$\&\textbf{max\_id}=$\langle$tweet id$\rangle$
            \end{center}
         \end{minipage}
      }
      \captionfonts
      \caption[Twitter API Query]{The structure of a typical query to the Twitter API.}
      \label{fig:apiQuery}
   \end{center}
\end{figure}

The parameters are:

\begin{description}

\item[json:]
Twitter can supply the result data in either ATOM or JSON format. Testing with both have shown that the ATOM
results are less consistent and provide less data. Because of the more accurate information returned from the JSON
API, we are able to write more efficient queries. Using the ATOM API, we could query Twitter only once every five
minutes; as opposed to every two minutes with the JSON API.

\item[q:]
The search query. Twitter supports some advanced search features such as conjunction and negation.

\item[rpp:]
``Results Per Page''. Twitter paginates the responses from the Search API. SPOONS always uses the maximum pagination value to decrease the number of requests per hour and lessen the chance of being rate limited.

\item[result\_type:]
Twitter allows users to get results ordered by either relevance or time. Since we want to gather all tweets about
our query, we choose to get the results ordered by time. In addition, the ``since\_id'' and ``max\_id''
parameters do not work when results are sorted by relevance.

\item[since\_id:]
The id of the oldest tweet that should be returned. This is not a hard limit, but provides a nice starting point.

\item[max\_id:]
The id of the most recent tweet that should be returned. It may seem counter-intuitive to provide a cap on the
most recent tweet, when one wants to query for all of the most recent tweets. However when a query's results spans across
more than 15 pages, it needs to be broken into a new query restarting at the first page. In this situation,
not providing an upper limit includes new tweets outside of the original search scope. This can result in tweets that are forever lost to us.

\end{description}

\section{Result Anatomy}
\begin{figure}
   \begin{center}
      \fbox{
         \begin{minipage}{160mm}
            \includegraphics[width=150mm]{images/api_result.eps}
         \end{minipage}
      }
      \captionfonts
      \caption[Twitter Search API Result]{A JSON result from the Twitter Search API}
      \label{fig:apiRes}
   \end{center}
\end{figure}

Figure~\ref{fig:apiRes} shows the result from the query ``eriq netflix''. Notice that some fields,
like the \textsf{geo} field, can be null. Also note that the API incorrectly guessed the language of the tweet as Danish.


%%%%%%%%%%%%%%%%%%  Arch  %%%%%%%%%%%%%%%%%%%%%


\part{SPOONS Architecture}
\label{arch}

\chapter{Architecture Breakdown}
\label{arch-breakdown}

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/SPOONS_Framework_Architecture.eps}
      \captionfonts
      \caption[SPOONS Framework Architecture]{The flow of control and data through the SPOONS framework system.}
      \label{fig:frameworkArch}
   \end{center}
\end{figure}

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/ui.eps}
      \captionfonts
      \caption[SPOONS UI]{The web UI for SPOONS.}
      \label{fig:ui}
   \end{center}
\end{figure}

There are multiple levels of architecture within SPOONS that need to be discussed.
Chapter~\ref{arch-framework} describes the framework architecture (Figure~\ref{fig:frameworkArch}).
The framework architecture describes the relations between the different pieces of the SPOONS framework.
Chapter~\ref{arch-dist} describes both the layout of the different servers involved in the SPOONS system
and the Distribution Model which describes how pieces of work are distributed between the different servers
Finally, Chapter~\ref{arch-database} discusses the architecture of the database that backs SPOONS.

\chapter{Framework Architecture}
\label{arch-framework}
This chapter describes the architecture of the SPOONS framework. The SPOONS framework includes all pieces of SPOONS
that take the data from gathering all the way through to final analysis.

\section{High Level Solution}
\label{arch-framework-highlevel}
The general solution taken by SPOONS consists of four main steps:

\begin{description}
   \item[Collect:]
      Collect tweets from Twitter.
   \item[Process:]
      Convert the tweets from plain text to some form of information that can be analyzed.
   \item[Model:]
      Use the information generated from the previous step to build a mathematical model of the information.
      Use past information to predict what the current model of the data should look like.
   \item[Compare:]
      Compare the two models generated in the previous step. A significant divergence means that there is
      anomalous traffic.
\end{description}

\subsection{Framework Overview}
\label{arch-framework-highlevel-overview}

Figure~\ref{fig:frameworkArch} shows the flow of control and data through the SPOONS framework. Data comes into SPOONS
in the form of Tweets collected by the Gatherers, and leave SPOONS in the form of alerts generated by the Monitors.

\paragraph{Gatherer.}
Gatherers are responsible for collecting documents from a specified data source such as the Twitter Search API.

\paragraph{Database.}
After the tweets are gathered, they are placed in the database. In addition to storing just tweets, the database also stores
configuration data, intermediate calculations, and the results of the Analysis Pipelines.

\paragraph{Control.}
The Control is responsible for controlling the SPOONS server. It maintains data structures with all of the Gatherers and
Analysis Pipelines. It is also responsible for communication with other servers in the SPOONS cluster.

\paragraph{Processor.}
Processors are data transformation utilities that take raw data and puts it in a form that other components can use.

\paragraph{Modeler.}
Modelers are responsible for building a mathematical model of the data and can be split into two groups: \textbf{Predictors} and \textbf{Counters}.
Predictors build a predictive model of the data. Counters build a model of the data that was actually gathered by the system.

\paragraph{Monitor.}
Monitors take the models produced by the Predictors and Counters and compares them. The Monitors are responsible for
making the final decision on about a period of time being anomalous.

\section{Gatherers}
\label{arch-gatherers}
The data enters SPOONS at the Gatherers. The Gatherers run periodically (for Twitter, every two minutes).
Gatherers are asynchronous and not dependent on any other part of the framework. There may be multiple different
Gatherers running on the same machine. Gatherers are abstracted to be able to gather data from any source.
Once the Gatherers obtain their data, they place the data in the database and notify the Control that there is new data
available to the system.

\subsection{Twitter Holes}
\label{arch-twitter-holes}
It is worth noting that sometimes the Twitter Search API fails to return any data. We have not discovered the cause
of this, but Twitter does not report any errors. For unspecified amounts of time the Twitter API reports zero
new tweets. We call these dead zones ``holes''. We have found that a query from a different IP usually does not
experience the same hole. To counteract holes, we run Gatherers on multiple servers and resolve duplicate tweets upon insertion
into the database.

\begin{figure}
   \begin{center}
      \includegraphics[width=0.9\textwidth]{images/Twitter_Holes.eps}
      \captionfonts
      \caption[Twitter Holes]{One server in a hole is covered by two other gathering servers.}
      \label{fig:twitterHoles}
   \end{center}
\end{figure}

\section{Processors}
\label{arch-processors}
Processors are responsible for processing or transforming data before it goes into the analysis pipelines.
\begin{itemize}
   \item \underline{Classifier Processors}: There exists a Processor for every tweet classifier used in SPOONS (see Chapter~\ref{classifiers}).
   Because of the high number of classifiers used, these constitute the majority of Processors and form the largest unit of work in SPOONS.
   These Processors classify every tweet into one of the nine tweet categories discussed in Section~\ref{class-tweet-classes}.

   \item \underline{Author Processors}: The Author Processors extract the author of tweets and try to establish which authors are credible. These Processors are
   outside the scope of this work and are discussed in other work\cite{cailinThesis}.

   \item \underline{Valence Processors}: The Valence Processors assign a numeric ``happiness'' score to every tweet. How that score is produced is outside the
   scope of this work (see Section~\ref{future-work-kim}).

   \item \underline{Document Frequency Processors}: The Document Frequency Processors maintain term frequencies and inverse document frequencies for the collection
   of tweets in SPOONS.
\end{itemize}

\paragraph{Implementation Notes:}
Unlike most parts of the analysis pipeline, Processors are a shared resource. That is, multiple analysis pipelines
invoke the same Processors. However, it does not make sense to restart the processing once it is started, or to
start another instance of the same Processor for the same data. Processors have a finite amount of data to process and may be cumulative.
To make sure that no redundant work is done, Processors are singleton. When multiple threads call into a Processor to do work, the Processor blocks
all incoming threads until the work is complete. Then, the Processor releases all of the threads that requested the work.
This model allows all the analysis pipelines to share the same Processor without any redundancies.

\section{Analysis Pipelines}
\label{arch-pipelines}
An Analysis Pipeline (also called Analysis Method) is the analytical center of the SPOONS framework.
Pipelines are split into Tasks (see Section~\ref{arch-tasks}) which are chunked units of work.
The exact number and types of Tasks used are different for each pipeline.

The run of an Analysis Pipeline is typically as follows:
\begin{enumerate}
   \item Pre-process incoming traffic.
   \item Model the existing traffic.
   \item Predict what the current traffic should be.
   \item Raise an alert if the existing traffic varies significantly from the predicted traffic.
\end{enumerate}

Every Analysis Pipeline gets its own thread, and there is no interdependence between the different pipelines.
Currently, SPOONS usually runs more than 20 Analysis Pipelines at a time.

\section{Tasks}
\label{arch-tasks}
Tasks are the core unit of computation in SPOONS. Almost everything that can be ``run'' is a Task.
Every Task gets its own thread, and callers into the Task may request that the Task block the calling thread
until the Task is complete.

\paragraph{Implementation Notes:}
Tasks are singleton with respect to the leaf child class.
This ensures that although there there are many different Tasks, every Task can be uniquely referenced.
This singleton behavior is enforced by checking the fully-qualified class name in the Task base class upon construction.
The uniqueness of tasks is very important to SPOONS distribution model discussed in Chapter~\ref{arch-dist}.

\section{Modelers}
\label{arch-modelers}
Modelers are Tasks that are responsible for building a mathematical representation for the data.

\subsection{Predictors}
\label{arch-predictors}
Predictors build a predictive model of the data. For example, we have noticed that tweet volume tends to be
periodic day-to-day and week-to-week. Therefore, a Predictor may model that prediction by guessing that the volume
in the future will be the same as it was the previous week or day.

\subsection{Counters}
\label{arch-counters}
Counters attempt to build a model of data that was actually gathered by the system. Going with the previous example,
the Counter for modeling tweet volume would simply count the number of tweets gathered for a period.

\section{Monitors}
\label{arch-monitors}
Monitors take the models produced by the Predictors and Counters and compares them one point at a time.
If the two models differ significantly, then an alert is raised.
The different types of Monitors are described in detail in Section~\ref{outage-detection-monitors}.
The Monitors are responsible for making the final decision about a period of time being anomalous.

\subsection{Auto-Tuning}
\label{arch-autotuning}
Monitors are the most configurable part of the Analysis Pipeline taking anywhere from two to six configurable parameters.
To find the best set of parameters, the Monitors can automatically run themselves on a training set and search the space of all possible parameters.
They then keep the parameters that result in the best score.
This process is called ``auto-tuning''.

\subsection{Resistance}
\label{arch-resistance}
At any given time, a Monitor is either in a normal, non-alerting, state or an alerting state.
A Monitor's ``resistance'' is its tendency not to move into or out of an alerting state.
The resistance is the number of normal or abnormal observations it needs to be
trigger a state change. Monitors are given resistance because otherwise an outliers could cause
a Monitor to rapidly switch between alerting and normal states.
There currently are three different methods of observing resistance.
The method of resistance as well as the resistance thresholds can also be auto-tuned.

\subsubsection{Fighting Resistance}
\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|p{9cm}|c|}
         \hline
            Parameter & Description & Restrictions \\
         \hline
            A & The number the counter must reach to enter an alerting state. & $ A > 0 $ \\
         \hline
            R & The number the counter must reach to enter a normal state. & $ R > 0 $ \\
         \hline
      \end{tabular}
   \end{center}
\end{table}

Fighting resistance counts every time that there is a normal period as a +1, and every time there is
an anomalous period as a -1. If the counter reaches $-A$, then the Monitor is put into an alerting state.
If the counter reaches $R$, then the Monitor is put into a normal state.

\subsubsection{Continuous Resistance}
\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|p{9cm}|c|}
         \hline
            Parameter & Description & Restrictions \\
         \hline
            A & The number the counter must reach to enter an alerting state. & $ A > 0 $ \\
         \hline
            R & The number the counter must reach to enter a normal state. & $ R > 0 $ \\
         \hline
      \end{tabular}
   \end{center}
\end{table}

Continuous resistance must get $A$ continuous anomalous observations to enter an alerting state, and $R$ continuous normal
observations to enter a normal state.

\subsubsection{Window Resistance}
\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|p{9cm}|c|}
         \hline
            Parameter & Description & Restrictions \\
         \hline
            W & The window size. & $ W > 0 $ \\
         \hline
            C & The number of anomalous observations necessary for an alerting state. & $ 0 < C <= W $ \\
         \hline
      \end{tabular}
   \end{center}
\end{table}

Window resistance remembers $W$  previous observations as being normal or anomalous. If the number of anomalous
observations is or exceeds $C$, then an alerting state is declared. Otherwise, the Monitor stays in a normal state.

\subsection{Smoothers}
\label{arch-smoothers}
The Monitors have a chance to smooth the data before it gets analyzed.
Smoothers take in a stream of data.
As with resistance methods, different smoothers and smoothing parameters can be auto-tuned.

\subsubsection{No Smoother}
Do not smooth. If this smoother is put into the parameter search space, then the effects of no smoothing can be seen.

\subsubsection{Moving Mean Smoother}
\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Parameter & Description & Restrictions \\
         \hline
            W & The window size. & $ W > 0 $ \\
         \hline
      \end{tabular}
   \end{center}
\end{table}

The Moving Mean Smoother works by taking the mean in a sliding window of size $W$.
This Smoother tolerates a smaller window if there is not enough data available.
Therefore, this Smoother always outputs a number for every number in the input stream.

\section{Control}
\label{arch-control}
The Control is the center of a SPOONS instance. It handles the flow of all control and has the ability to start and
stop any Task or Analysis Pipeline on demand. It holds references to all the threads for the Gatherers and Analysis Pipelines.
The Control handles all the setup and tear down in the system.

There are different types of Controls that decide the behavior of SPOONS on each respective server.
There are three types of Controls: \textbf{Master Control}, \textbf{Worker Control}, and \textbf{Single Control}.
The Control is singleton with respects to the base class.
Therefore, only one instance of any type of Control can be active on a server at any given time.

\paragraph{Implementation Notes:}
The Control is very careful to never allow anyone to own a reference to the currently running Control.
All requests to the Control are made statically to the ``Control'' base class.
The base class then forwards the request onto the specific instance of Control currently active on the server.
This is done so that the rest of the SPOONS system never knows what kind of Control is currently active.
Because of that, a server can be switched between different roles without restarting the system or notifying any other components of the SPOONS system.

\subsection{Master Control}
\label{arch-master-control}
The Master Control is the Control that is responsible for the controlling SPOONS when it is in distributed mode.
The Master Control maintains information on all the active worker servers including what Tasks are currently assigned to them.

\paragraph{Implementation Notes:}
The Master Control maintains ``shallow execution'' of every pipeline in the system.
This means that this control runs each pipeline, but then distributes work for each pipeline as the work is generated.

\subsection{Worker Control}
\label{arch-worker-control}
Worker Controls do not take any initiative to run any tasks. Instead, they just wait for a Master Control to tell them what to do.

\subsection{Single Control}
\label{arch-single-control}
The Single Control is for a SPOONS instance that wants to run on a single server.

\chapter{Distributed Computation Model}
\label{arch-dist}

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/SPOONS_Server_Architecture.eps}
      \captionfonts
      \caption[SPOONS Server Architecture]{The server architecture of the SPOONS system.}
      \label{fig:serverArch}
   \end{center}
\end{figure}

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/SPOONS_Distributable_Task_Control_Flow.eps}
      \captionfonts
      \caption[SPOONS Distributable Task Flow]{The control flow for distributable tasks.}
      \label{fig:taskFlow}
   \end{center}
\end{figure}

As discussed before, SPOONS is a multi-server system (Fig~\ref{fig:serverArch}).
The SPOONS system uses the master/worker paradigm with a single master and $N$ workers.

All of the servers share two resources: the primary database and a NoSQL property store.
When a master or worker comes online, it inserts an entry for itself into the shared property store.
If the new server is a worker, it alerts the current master about its existence, and visa-versa,
if the new server is a master. In addition, all workers are required to heartbeat to the master every
15 seconds and the master heartbeats to the workers every 15 seconds. Using this system, the master
always knowns about all of the workers and the worker always knows about the current master.
When a server misses three heartbeats, the server expecting that heartbeat assumes that the server has
gone down.

\section{Distribution Requirements}
\label{arch-dist-requirements}
The design of the SPOONS distribution model was dominated by two main concerns: performance and usability.

\paragraph{Performance.}
SPOONS is a real-time system. Any attempt at distribution cannot compromise the response time of the system.
In addition, SPOONS has to be able to survive a server dying. These two concerns led to three requirements:

\begin{itemize}
   \item Efficiency: A distributed SPOONS should have comparable response time to a single server SPOONS.
   \item Fault Tolerance: SPOONS should be able to survive any non-database server failing.
   \item Scalability: Scaling a SPOONS cluster should be straightforward.
\end{itemize}

\paragraph{Usability.}
SPOONS is meant to be a general real-time analysis system that many different types of people can use.
The average developer using SPOONS should be concerned only with the framework level.
Server, network, and distribution semantics should remain transparent to a user of the framework.
To make a distributed SPOONS easy to use for a developer working with the framework, three requirements
are stipulated:

\begin{itemize}
   \item DB: There should be one database, and a framework developer should not have to do anything that is not required on a simple single-server system.
   \item Framework Complexity: All distribution specifics should be hidden from the framework developer.
   \item Single Mode: SPOONS should be able to run on a single server.
\end{itemize}

\section{Distribution Assumptions}
\label{arch-dist-assumptions}
The SPOONS distribution model relies on two assumptions about the system: every server contains
exactly the same data in memory and every Task can be uniquely referenced.

\subsection{Same Data}
SPOONS assumes that every server has the same data in memory on every server.
This means that not only does every server need to have the same data structures in memory,
but also that every server needs to have the same classes instantiated. The only exception to this
assumption is the Control. Depending on the role of the server, a different Control is
instantiated. Because of this assumption, we do not have to worry about active replication
between servers or a worker being asked to do work that requires a class that is not instantiated.

\subsection{Uniquely Referenced Tasks}
As stated in Section~\ref{arch-tasks}, Tasks are the basic unit of work inside SPOONS.
When a worker is told to execute some work, it is being asked to execute a specific Task with specified
parameters. Therefore, Tasks need to be able to be referenced by a key that can be serialized and
sent over the wire from the master to the worker.

\section{Distributable Tasks}
\label{arch-dist-tasks}
Distributable Task is a subclass of Task that provides the distribution mechanism for Tasks.
When a Task is to be distributed, the Distributable Task calls into the Control and requests that
the Control distributes it. The next step varies depending on the type of Control that is active:

\subsection{Master Control}
The Task distributing control flow is described in Figure~\ref{fig:taskFlow}.
A Master Control blocks the calling thread and send a message to a selected worker\footnote{The
current scheduling algorithm chooses the worker that has the fewest Tasks currently assigned to it.} telling it to
run the Task with given parameters. The message that goes to the worker just contains the Task's unique identifier
and the parameters to the Task's run. When the Task is complete, the Worker Control sends the Task's return
status back to the Master Control. When the master receives a message from the worker that the requested Task
has completed its run, it resumes the original calling thread and have it return with the return status given
by the worker.

\subsection{Worker Control}
Worker Controls do not distribute Tasks.
Because Tasks are atomic units of work, Tasks are not supposed to call other Tasks.
If Task A calls upon a Worker Control to distribute a Task B, then that means that Task A has violated
its own atomicity. This is considered a violation of the framework and causes the Worker Control to throw an error.

\subsection{Single Control}
Instead of blocking the calling thread like in the Master Control, a Single Control
just uses the calling thread to run the Task. When the Task is complete,
the Single Control returns control to the caller.

\section{Shared Properties}
\label{arch-props}
As previously stated, all servers must maintain a consistent in-memory view of the system.
This can be troublesome if a Task needs to maintain cumulative settings or member datum.
Not only will this data need to be consistent on all the servers, but it also needs to maintain this
data between starts and stop of the system.
An Analysis Pipeline should be able to be interrupted at any moment, and then restarted later without losing data or its place.

To enforce these restrictions, SPOONS uses a shared property store. The shared property store is a MongoDB server.
Whenever a Task needs to store member datum, it places the data in the shared store, making the data available to any server in the cluster.
A Task can first be run to completion on Server A and then, when new data is available, run on Server B.
Because the Task stores the necessary information in the shared property store,
Server B can have all the information gained from the run on Server A and not lose any positional information.

In addition to storing shared properties, the shared property store houses information on every active server.
When a server comes online, it queries the property store to find all the other active servers and inserts itself into
the store. If a server fails to heartbeat, then the rest of the cluster that is still active removes the entry for that server from the property store.

\chapter{Database}
\label{arch-database}
SPOONS is backed by a MySQL database. SPOONS currently uses 225 tables and 35 stored procedures. The 225 tables are further divided into
six different categories that are used in different stages of the analysis pipeline. In addition to tweets,
configuration data, intermediate calculations, analysis results, and final alerting decisions are stored in the database. Keeping all of this
data allows the users to look back at any point in time for reference or debugging.

The database uses naming and schema conventions to maintain organization on its tables. The naming and schema conventions allow different
components of the Analysis Pipeline to be interchanged without any need to change/reprocess the data. In addition the conventions allows the UI
to represent new tables without the need for specifying them.

\section{Tables and Schemas}
\label{arch-database-tables}
Each stage in an analysis pipeline generally stores some information in the database. Because each stage generally deals with similar
types of data, these tables are considered to be in the same group. We enforce group membership using hints in the table names. For example, the
table name ``\texttt{RESULT\_EN\_class\_heuristic\_bayes\_net}'' gives five hints as to the type of the table.

\begin{enumerate}
   \item \texttt{RESULT} - Marks this table as a result table. This means that it is guaranteed to be shown in the UI.
   \item \texttt{EN} - The language of the tweets that were input into this method.
   \item \texttt{class} - Indicates that this these results are output from a tweet classifier.
   \item \texttt{heuristic} - States that the type of classifier used was a heuristic classifier.
   \item \texttt{bayes\_net} - The name of the classifier used.
\end{enumerate}

Using all of these hints, the UI can then ask for data for specific types of tables (eg. all result tables that are for English tweets).

The six different top level categories that SPOONS recognizes are:

\begin{enumerate}
   \item CALC - These tables store intermediate results in analysis pipelines. CALC tables are typically only used when large sets of past data are needed for cumulative models. They are never shown to the UI.
   \item CONFIG - Contains information that analysis methods used to configure themselves before runs. These tables have been mostly replaced with the shared property store (see Section~\ref{arch-props}).
   \item DATA - Raw input data. These tables are generally the output from the Gatherers.
   \item META - Contains information that is not analyzed, but required by the system. For example, the different classes that the classifiers use along with descriptions of each class.
   \item RESULT - These tables are output from some analysis pipeline. They are guaranteed to be shown in the UI. These tables typically contain time series of some analytical signal.
   \item TEST - These tables are used for debugging and development. They are never shown in a user-facing UI, however may be shown in development UIs.
\end{enumerate}

The full schemas for select tables are described in Appendix~\ref{appendix-db-schema}.

\subsection{Data Flow}
\label{arch-database-data-flow}
The flow of data through the different types of tables is described in Figure~\ref{fig:db-data-flow}.
The data originates from the Gatherers and is moved into DATA tables. Information from
DATA, CONFIG, and META tables are analyzed and placed in either CALC, RESULT, or TEST tables.
At a later time the data from CALC tables is further analyzed and the results are placed in a RESULT table.

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/DB_Data_Flow.eps}
      \captionfonts
      \caption[Database Data Flow]{The flow of data through the different types of tables in SPOONS.}
      \label{fig:db-data-flow}
   \end{center}
\end{figure}

\subsection{Tweets Table}
\label{arch-database-tables-tweets}
As the most used and important table in the database, the table that houses all of our tweets, ``DATA\_tweets'', gets special attention.

The tweets table contains ten attributes which are described in Table~\ref{table:tweet-attributes}.

\begin{table}
   \begin{center}
      {\Large Data\_tweets Schema}
      \begin{tabular}{|l|p{12cm}|}
         \hline
            Name & Description
         \tabularnewline\hline
            \textsf{id} & An auto-incremented primary key.
         \tabularnewline\hline
            \textsf{twitter\_id} & The unique id Twitter gives to a tweet.
         \tabularnewline\hline
            \textsf{published} & The epoch time that the tweet was posted according to Twitter.
         \tabularnewline\hline
            \textsf{content} & The raw content of the tweet.
         \tabularnewline\hline
            \textsf{source} & Information on where the tweet was posted from (eg. from a third party app).
         \tabularnewline\hline
            \textsf{lang} & The suggested language of the tweet.
         \tabularnewline\hline
            \textsf{author} & The author of the tweet.
         \tabularnewline\hline
            \textsf{frame\_id} & The frame that this tweet falls into, has an index on it.
         \tabularnewline\hline
            \textsf{place} & Information on where the tweet was posted from. This is a JSON structure and may contain fields such as ``city'' and ``state''.
         \tabularnewline\hline
            \textsf{geo} & Geographical coordinates of place.
         \tabularnewline\hline
      \end{tabular}
   \end{center}
   \caption[Database Tweet Attributes]{The database attributes used to describe tweets.}
   \label{table:tweet-attributes}
\end{table}

\subsubsection{Frames}
Inside SPOONS, we use a ``frame'' as the atomic unit of time. Currently, a frame corresponds to a minute. Bucketing the tweets into frames allows us to
gain a natural aggregation and smoothing. It also provides a natural index. Maintaining an index on \textit{frame\_id} allows quick retrieval of
time series data which is the primary task of SPOONS. Because insertions are generally chronological, insertions are also quick and do not require a
rebuild of the B-Tree index\cite{innodb}.

\section{UI Stored Procedures}
\label{arch-database-sp}
In addition to utility procedures, the database holds many stored procedures used by the UI.
This keeps the UI fairly stable in the face of database changes.

\subsection{Expected Schemas}
\label{arch-database-sp-schemas}
The UI Stored Procedures look for 6 distinct name/schema combinations all of which are required to have the ``\texttt{RESULT}'' prefix.
The different schema requirements are shown in Table~\ref{table:ui-expected-schema}, and described below:

\begin{table}
   \begin{center}
      \begin{tabular}{|l|p{12cm}|}
         \hline
            Schema Name & Required Columns
         \tabularnewline\hline
            Volume & \textsf{start\_frame}, \textsf{value}
         \tabularnewline\hline
            Volume Prediction & \textsf{start\_frame}, \textsf{prediction}
         \tabularnewline\hline
            Valence & \textsf{start\_frame}, \textsf{value}
         \tabularnewline\hline
            Valence & \textsf{start\_frame}, \textsf{prediction}
         \tabularnewline\hline
            Class & \textsf{start\_frame}, \textsf{undecided}, \textsf{media}, \textsf{neutral}, \textsf{snafu}, \textsf{watching}, \textsf{response}, \textsf{complaint}, \textsf{refuse\_to\_rate}, \textsf{happy}
         \tabularnewline\hline
            Group & \textsf{start\_frame}, \textsf{media}, \textsf{bad}, \textsf{other}
         \tabularnewline\hline
      \end{tabular}
   \end{center}
   \caption[Stored Procedure UI Expected Schema]{The different types of schemas that the UI looks for in RESULT tables.}
   \label{table:ui-expected-schema}
\end{table}

\begin{description}
   \item[Volume.]
   This schema is for tables that contain time series information about tweet volumes.
   This includes tables that hold the time series for the total Netflix-related Twitter traffic.

   \item[Volume Prediction.]
   These tables contain time series that are predictive models of Netflix-related Twitter traffic.

   \item[Valence.]
   These tables contain time series for estimates of the current sentiment about Netflix.

   \item[Valence Prediction.]
   These tables contain time series that are predictive models of the sentiment about Netflix.

   \item[Class.]
   These tables contain time series for the volume of tweets that were classified into
   each of the nine categories described in Section~\ref{class-tweet-classes}.

   \item[Group.]
   These tables contain time series for the volume of tweets that were classified into
   each of the thee different groupings described in Section~\ref{class-tweet-groups}.
\end{description}

The stored procedures further divides the tables by language. The currently recognized languages are English,
Spanish, and Portuguese.

\part{Analysis}
\label{analysis}

\chapter{Classifiers}
\label{classifiers}

\section{Why Classification?}
\label{class-why}
Classification helps discover Netflix service outages by differentiating between different types of Twitter traffic.

Figure~\ref{fig:normal-traffic} shows the normal pattern of Netflix-related Twitter traffic over the course of a single week.
The peaks appear at around 7pm PST and the valleys are around 2am PST.
This kind of pattern is very regular and repeats weekly during normal times.
However, where there is some sort of event, the traffic develops spikes. Figure~\ref{fig:anomalous-traffic} shows a period with two
anomalous spikes. However, sampling tweets from the different spikes hints that the causes for the two spikes
are very different. Figure~\ref{fig:linkless-traffic} shows tweets sampled from each spike. The left spike is composed mostly
of tweets indicating that Netflix is experiencing a service outage. The right spike however, is composed mainly of tweets linking to
a news article about Netflix. Therefore, we see that not only service outages generate spikes in Netflix-related Twitter traffic.

This is where classifiers become useful. If tweets can be placed into different classes according to their type,
then the different types of traffic can be differentiated. Figure~\ref{fig:classified-traffic} shows the result of classifying
the tweets and then building time series of the classes respective traffic. It becomes obvious that the spike on the left is caused by
outage related traffic and that the spike on the right is caused by media related traffic.

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/Normal_Traffic.eps}
      \captionfonts
      \caption[Normal Traffic]{A week's worth of Netflix-related Twitter traffic. Notice the daily periodicity.}
      \label{fig:normal-traffic}
   \end{center}
\end{figure}

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/Anomalous_Traffic.eps}
      \captionfonts
      \caption[Anomalous Traffic]{Netflix-related Twitter traffic with two different anomalies.}
      \label{fig:anomalous-traffic}
   \end{center}
\end{figure}

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/Linkless_Differentiation.eps}
      \captionfonts
      \caption[Linkless Anomalous Traffic]{The same traffic shown in Figure~\ref{fig:anomalous-traffic}, with an additional line showing Netflix-related Twitter traffic that does not contain a URL.}
      \label{fig:linkless-traffic}
   \end{center}
\end{figure}

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/Classified_Traffic.eps}
      \captionfonts
      \caption[Classified Traffic]{The same traffic shown in Figure~\ref{fig:anomalous-traffic}, with two addition lines: the volume of tweets classified as ``Bad'' and the volume of tweets classified as ``Media''.}
      \label{fig:classified-traffic}
   \end{center}
\end{figure}

\section{Classification Roadmap}
\label{class-roadmap}
The steps that SPOONS takes to use classification to detect service outages can be divided into two categories:
prep work and online.

The prep work includes:

\begin{enumerate}
   \item Observe Netflix-related Twitter traffic and observe the different classes that the tweets fall into.
   \item Build a training set biasing anomalous traffic.
   \item Classify incoming tweets.
   \item Group classified tweets according to the type of traffic that class produces.
   \item Establish the best classifiers.
\end{enumerate}

After the off-line prep work is complete, the online analysis can begin:

\begin{enumerate}
   \item Use the best classifiers in an Analysis Pipeline.
   \item Observe the differences between the total traffic and the traffic classified as anomalous.
   \item Declare an outage when the two traffics diverge significantly.
\end{enumerate}

\section{Tweet Classes}
\label{class-tweet-classes}
After observing Netflix-related Twitter traffic, we decided tweets fall into at least one of nine different categories.

\begin{itemize}
  \item \texttt{Media} -- Tweets relating to a media story about Netflix. Typically a link to a news article.
  \item \texttt{Snafu} -- Tweets that talk about a Netflix outage.
  \item \texttt{Complaint} -- Tweets where people are complaining about Netflix.
  \item \texttt{Happy} -- Tweets that expresses the user's joy about Netflix.
  \item \texttt{Neutral} -- Tweets that are just a neutral observation or comment about Netflix.
  \item \texttt{Watching} -- Tweets that gives updates about what the user is currently watching.
  \item \texttt{Response} -- Tweets that are a neutral response to another user in a Netflix-related conversation.
  \item \texttt{Refuse To Rate} -- Tweets that we we refuse to rate entirely (usually tweets that are in a different language than the training set).
  \item \texttt{Undetermined} -- This class does not exist in the wild. It is used during classification as default for all tweets that don't match any other class.
\end{itemize}

Examples of tweets with their corresponding classes are shown in Table~\ref{table:classes}.

\begin{table}
   \begin{center}
      \begin{tabular}{|l|p{12cm}|}
         \hline
            Class & Tweet Example
         \tabularnewline\hline
            \texttt{Media} & \texttt{Netflix Now Available Through Facebook - http://bit.ly/ffpBHH - [Geeky Gadgets]}
         \tabularnewline\hline
            \texttt{Snafu} & \texttt{And netflix is broken. Why is this happening to me.}
         \tabularnewline\hline
            \texttt{Complaint} & \texttt{netflix keeps taking little things i like about the site away...Why?}
         \tabularnewline\hline
            \texttt{Happy} & \texttt{Netflix :)}
         \tabularnewline\hline
            \texttt{Neutral} & \texttt{about to download this netflix free trial}
         \tabularnewline\hline
            \texttt{Watching} & \texttt{Watching Family Guy on Netflix}
         \tabularnewline\hline
            \texttt{Response} & \texttt{@BeehiveBlog  Both good movies.  I think I'll put on the netflix list.}
         \tabularnewline\hline
            \texttt{Refuse To Rate} & \texttt{en serio, QUIERO pagar por algo como Netflix, DEJARME pagar}
         \tabularnewline\hline
      \end{tabular}
   \end{center}
   \caption[Tweet Class Examples]{Examples of the types of tweets that go with each class.}
   \label{table:classes}
\end{table}

\subsection{Tweet Groups}
\label{class-tweet-groups}
Because the goal of SPOONS is to detect anomalous traffic, it is useful to collapse the nine classes into
three different groups that account for the different types of Netflix-related traffic.

\begin{itemize}
  \item \texttt{Media}: Contains only the \texttt{media} class.
  \item \texttt{Bad}: Contains both the \texttt{snafu} and \texttt{complaint} classes.
  \item \texttt{Other/Normal}: Contains all other classes.
\end{itemize}

Figure~\ref{fig:groups} shows the amount of Netflix-related tweets during a Netflix outage and media event.
During normal times, the \texttt{normal} traffic is responsible for the majority of the overall traffic.
However during outage and media events, we see that the \texttt{bad} and \texttt{media} dominate the respective
periods.

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/groups.eps}
      \captionfonts
      \caption[SPOONS Groups]{The different volumes for different tweet classes during an outage (left) and media event (right).}
      \label{fig:groups}
   \end{center}
\end{figure}

\section{WEKA Classifiers}
\label{class-weka}
SPOONS uses several classifiers from the WEKA machine learning package\cite{weka}.
All of these classifiers have been discussed in Section~\ref{background-classifiers}.
The WEKA classifiers used are:

\begin{itemize}
   \item Naive Bayes
   \item Bayes Net
   \item J48 (a method of generating a C4.5 decision tree\cite{j48})
   \item K-Nearest Neighbors
   \item SMO (Support Vector Machine trained with Sequential Minimal Optimization\cite{smo})
\end{itemize}

\section{Non-WEKA Classifiers}
\label{class-nonweka}
In addition to the WEKA classifiers, SPOONS uses two classifiers implemented from scratch.
Because of low performance and inflexible API, the WEKA classifiers are being reimplemented.
As of now, only Naive Bayes has been reimplemented. The other classifier implemented from scratch is
a BPNB classifier which is discussed in Section~\ref{background-classifiers-bpnb}.

\section{Text Pre-Processing}
\label{class-processing}
Before the tweets are classified, they are pre-processed. During processing, the text is transformed
to make classification of the text easier. Standard text operations like stemming, stopword removal,
and case normalization; as well as Twitter and Netflix specific operations like hashtag and movie title
recognition are preformed. After the text is processed, it is split into unigrams to be used as features
in the classifiers.

\subsection{Text Filtering}
\label{class-filter}
Before the input text is split into features, it goes through heavy pre-processing.
The text filtering involves normalizing the case, remove extra characters, and replacing special features.

\subsubsection{Link Replacement}
\label{class-filter-link-replacement}
The first step in processing the text is to replace links.
Following a link may provide information about a tweet, however the link text of the link
itself provides no information. The presence of a link, however, can provide information about
a tweet.

\subsubsection{Twitter-Specific Symbols}
\label{class-filter-twitter-symbols}
Tweets often contain several special symbols specific to tweets.

\paragraph{RT.}
\label{class-twitter-symbols-rt}
``RT'' stands for ``re-tweet''. It means that the posted tweet is a repost of
a tweet made by another user. This symbol contains no reference to the original post.
``RT'' usually appears at the beginning of the tweet. For example, after the comedian
Conan O'Brien posted the following tweet:

\begin{center}
   \texttt{If I'm ever a ghost, I hope the person I haunt has Netflix.}
\end{center}

There were hundreds of identical tweets that said:

\begin{center}
   \texttt{RT: If I'm ever a ghost, I hope the person I haunt has Netflix.}
\end{center}

\paragraph{\#.}
\label{class-twitter-symbols-hash}
A ``\#'' (pronounced ``hashtag'') is used to mark keywords or topics.
Users can search for tweets by hashtag and see the collection of tweets supposedly about the
same topic. A hashtag does not have to reference a pre-existing topic.

\paragraph{@.}
\label{class-twitter-symbols-at}
An ``@'' in Twitter, simply pronounced ``at'', is a reference to another Twitter user.
A reference to a user alerts that user about the posted tweet.
For example, the following tweet references my Twitter account.

\begin{center}
   \texttt{Hi there, @eriqaugustine}
\end{center}

\subsubsection{Emoticon Parsing}
\label{class-filter-emoticon}
Emoticons are parsed out and replaced with meta words.
SPOONS emoticon parser was written by Ryan Hanarkis and Allen Dunlea as part of a project for Graduate Artificial Intelligence course.
Emoticons provide a plethora of information about a tweet. Sarcasm aside,
an emoticon can surmise the sentiment of an entire tweet.

\subsubsection{Title Replacement}
\label{class-filter-title}
Because our tweets are always about Netflix, a television show and movie streaming service,
titles are a common occurrence. However, movie and show titles often contain words that can be
detrimental to our analysis. For example, ``Death At A Funeral'' is the title of a movie, but contains
two words that have very negative connotations: ``death'' and ``funeral''.

Without title replacement, the following tweet would be very difficult to classify:

\begin{center}
   \texttt{Death at a Funeral is hilarious!  \#netflix}
\end{center}

However after title replacement, the tweet becomes very easy to classify:

\begin{center}
   \texttt{$\langle$\$title\$$\rangle$ is hilarious!  \#netflix}
\end{center}

SPOONS contains a table that has over 50,000 movie and show titles on Netflix. The titles were gathered
using the Netflix API. On startup, SPOONS builds a trie (prefix tree)\cite{trie} of all of the titles. In this trie titles can
only be split on a word level, not on a character level. Therefore, moving to the next node consumes a single word.
Searching for a title becomes a simple walk down the trie.
If the walk of the trie ends on a terminal node, then a title is found. If not, then the trie is walked again from the beginning
starting at the next word in the tweet. Figure~\ref{fig:title-trie} shows a sample walk in the title trie.

% TODO(eriq): [Lowest Priority] Formalize algorithm with pseudocode.

\begin{figure}
   \begin{center}
      \includegraphics[width=0.8\textwidth]{images/Title_Trie.eps}
      \captionfonts
      \caption[Title Trie Walk]{A sample trie of Frankenstein movie titles. The solid lines show what nodes the search for "Frankenstein Meets The Wolf Man" would traverse.}
      \label{fig:title-trie}
   \end{center}
\end{figure}

\subsubsection{Stemming}
\label{class-filter-stemming}
Stemming finds the root of a word. This allows words to be categorized by their roots which
decreases the number of unique words being evaluated and emphasizes linguistic patterns.
This preprocessor uses Porter's stemmer for the English language \cite{porters}.

\subsubsection{Stop Word Removal}
\label{class-filter-stopword}
Stopwords, or words that carry little or no semantic information, are identified based
on a static table of words mapped to levels. Stopwords are assigned levels which allow
processes to use different sets of stop words. All words less than 3 character are also
automatically considered stop words.

\subsubsection{Punctuation/Non-English Character Removal}
\label{class-filter-noneng}
Removes all punctuation and
characters not in the English alphabet. This simplifies word extraction and
comparison.

\subsubsection{Meta Words}
\label{class-filter-meta}
Below is an overview of meta words that SPOONS recognizes:

\begin{description}
   \item[$\langle$\$title\$$\rangle$] Indicates the presence of a movie or show title.
   \item[$\langle$\$link\$$\rangle$] Indicates the presence of a URL.
   \item[$\langle$\$emote:*\$$\rangle$] Replaces an emoticon.
   \item[$\langle$\$RT\$$\rangle$] Indicates that a tweet is a ``retweet'' (a repeat of another tweet).
   \item[$\langle$\$\#\$$\rangle$] Inserted when a ``hashtag'' is found in a tweet. The original subject of the hashtag is separated off into another word. E.g. ``\#Netflix'' becomes ``$\langle$\$\#\$$\rangle$ Netflix''.
   \item[$\langle$\$@\$$\rangle$] Inserted when a reference to another Twitter user is made. The user that is the subject of the reference is separated off into another word.
\end{description}

\section{Training Set}
\label{class-training-set}

The classifiers were trained on a small set of \textbf{759} tweets which were pulled from from periods of both normal and
anomalous traffic. Each tweet in the training set was manually classified by multiple researchers until consensus
about the classification was reached. Because the goal is anomalous traffic detection, the training set over-samples the
tweets from \texttt{media}, \texttt{snafu}, and \texttt{complaint}: categories. Table~\ref{table:classCounts} documents
the structure of the training set and shows the  number of tweets classified into each of the eight categories.
Tweets were allowed to belong to multiple classes because of posts like, ``\texttt{I love netflix! Watching Law and Order
online!}'', which could be classified as both \texttt{happy} and \texttt{watching}.

See Appendix~\ref{appendix-full-training} for the full training set.

\begin{table}
   \begin{center}
      \begin{tabular}{|l|c|c|c|}
         \hline
         Class  & \# Tweets & Class & \# Tweets
         \tabularnewline\hline
         \texttt{Media} & 103 & \texttt{Neutral} & 66
         \tabularnewline\hline
         \texttt{Outage} & 158  & \texttt{Watching} &  135
         \tabularnewline\hline
         \texttt{Complaint}  & 146 &  \texttt{Response} &  30
         \tabularnewline\hline
         \texttt{Happy}  & 147  & \texttt{Undetermined}  & 48
         \tabularnewline\hline
      \end{tabular}
      \caption[Netflix-related Twitter Traffic]{Overview of the Netflix-related Twitter post training set used to train classifiers in SPOONS.}
      \label{table:classCounts}
   \end{center}
\end{table}

\section{Evaluation}
\label{class-evaluation}
Each classifier is individually evaluated just on its ability to classify tweets against the training set.
Each classifier varies the type of filtering it uses between no filtering and full filtering (every method discussed in Section~\ref{class-filter}).
Table~\ref{table:classifierEvaluationSpace} shows the combinations of classifiers and filters used for the classifier evaluation.
There will be 24 classifier/filtering combinations.

\begin{table}[H]
   \begin{center}
      \begin{tabular}{|l|}
         \hline
            \textbf{Classifiers}
         \tabularnewline\hline
            BayesNetClassifier
         \tabularnewline\hline
            BinaryBayesNetClassifier
         \tabularnewline\hline
            BinaryJ48Classifier
         \tabularnewline\hline
            BinaryKNNClassifier
         \tabularnewline\hline
            BinaryNaiveBayesClassifier
         \tabularnewline\hline
            BinarySMOClassifier
         \tabularnewline\hline
            J48Classifier
         \tabularnewline\hline
            KNNClassifier
         \tabularnewline\hline
            NaiveBayesClassifier
         \tabularnewline\hline
            Non-Weka BPNBClassifier
         \tabularnewline\hline
            Non-Weka NaiveBayesClassifier
         \tabularnewline\hline
            SMOClassifier
         \tabularnewline\hline
      \end{tabular}
      {
         \Huge
         $\times$
      }
      \begin{tabular}{|l|}
         \hline
            \textbf{Filtering}
         \tabularnewline\hline
            None
         \tabularnewline\hline
            Full
         \tabularnewline\hline
      \end{tabular}
      \caption[Classifier Evaluation Combinations]{The cross product of the classifier and filtering will be used to evaluate the classifiers.}
      \label{table:classifierEvaluationSpace}
   \end{center}
\end{table}

\subsection{Confusion Matrices}
\label{class-evaluation-confusion}
The results of the classification evaluation are shown in confusion matrices.
Table~\ref{table:confusionExample} shows the confusion matrix of the Naive Bayes classifier using full filtering as an example.
Down the vertical are the actual classes of the tweets, and across the horizontal are the predicted classes that the classifiers output.
Note that the example shows that this classifier incorrectly classified eight \texttt{media} tweets as another class (seen by looking down the \texttt{media} column),
and misclassified 22 non-media tweets as \texttt{media} (seen by looking across the \texttt{row}).

Overall accuracy for the classifier can be calculated by taking the number of correctly classified tweets (down the diagonal) over the total number of tweets.
Accuracy for a specific classification can be considered two ways: accuracy of classifying tweets \textbf{from} that class and accuracy of classifying \textbf{into} that class.
The accuracy of classifying tweets \textbf{from} a class can be seen by looking down a column.
The number correctly classified tweets (\textbf{bold}) over the total number of tweets in that column.
The accuracy of classifying tweets \textbf{info} a class can be seen using the same method as before, except looking across the class's row instead of its column.

\begin{table}[H]
   \begin{center}
      \scalebox{0.71}{
         \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
         \hline
            \diaghead{\theadfont Diag ColumnmnHead II}{Classified}{Actual} & media & neutral & snafu & watching & response & complaint & refuse to rate & happy \\
         \hline
            media & \textbf{81} & 3 & 0 & 1 & 3 & 0 & 14 & 1 \\
         \hline
            neutral & 0 & \textbf{23} & 3 & 14 & 11 & 11 & 1 & 3 \\
         \hline
            snafu & 0 & 7 & \textbf{81} & 8 & 4 & 55 & 2 & 1 \\
         \hline
            watching & 0 & 26 & 0 & \textbf{82} & 5 & 12 & 0 & 10 \\
         \hline
            response & 2 & 12 & 3 & 6 & \textbf{3} & 1 & 0 & 3 \\
         \hline
            complaint & 1 & 9 & 36 & 9 & 8 & \textbf{77} & 3 & 3 \\
         \hline
            refuse to rate & 5 & 4 & 1 & 2 & 4 & 9 & \textbf{21} & 2 \\
         \hline
            happy & 0 & 18 & 5 & 22 & 5 & 31 & 1 & \textbf{65} \\
         \hline
         \end{tabular}
      }
      \caption[Example Classification Confusion Matrix]{An example confusion matrix for a classifier. The \texttt{undecided} class was removed because there were no tweets in that class.}
      \label{table:confusionExample}
   \end{center}
\end{table}

\subsection{Results}
\label{class-evaluation-results}
Table~\ref{table:classification-summary-uncompressed} shows a summary of the results of the evaluation.
The SMO classifier took the top two spots with a top accuracy of \textbf{.5750}.
This is a decent accuracy, but much of the misclassification occurs between classes that don't matter
as much when trying to identify the different types of classes. For example, it does not matter if
a \texttt{watching} tweet is misclassified as a \texttt{happy} tweet. Both of those classes contribute to
normal background traffic. Table~\ref{table:confusionMisclassification} shows the confusion matrix for the top
ranked classifier (SMO with no filtering). All of the misclassification of the \texttt{other} group that
would become correct classification if the nine classes were compressed to the three groups are \textbf{bold}.
Because of this, the classifiers will be evaluated again, but the different classes
will be compressed into their respective groups before the results are evaluated.

\begin{table}[H]
   \begin{center}
      \scalebox{0.65}{
         \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
         \hline
            \diaghead{\theadfont Diag ColumnmnHead II}{Classified}{Actual} & undecided & media & neutral & snafu & watching & response & complaint & refuse to rate & happy \\
         \hline
            undecided& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
         \hline
            media    & 0 & 98 & 0 & 0 & 2 & 2 & 0 & 0 & 1 \\
         \hline
            neutral  & 0 & 3 & 11 & 2 & \textbf{\large 16} & \textbf{\large 7} & 7 & 0 & \textbf{\large 20} \\
         \hline
            snafu    & 0 & 0 & 1 & 87 & 4 & 4 & 41 & 0 & 21 \\
         \hline
            watching & 0 & 2 & \textbf{\large 12} & 3 & 88 & \textbf{\large 5} & 4 & 0 & \textbf{\large 21} \\
         \hline
            response & 0 & 2 & \textbf{\large 5} & 4 & \textbf{\large 7} & 0 & 3 & \textbf{\large 1} & \textbf{\large 8} \\
         \hline
            complaint& 0 & 2 & 5 & 40 & 4 & 1 & 68 & 1 & 25 \\
         \hline
            refuse to rate & 0 & 9 & \textbf{\large 1} & 1 & \textbf{\large 1} & \textbf{\large 1} & 0 & 18 & \textbf{\large 17} \\
         \hline
            happy    & 0 & 1 & \textbf{\large 9} & 3 & \textbf{\large 13} & \textbf{\large 6} & 8 & 0 & 107 \\
         \hline
         \end{tabular}
      }
      \caption[Uncompressed Misclassification Example]{The SMO results with no filtering. Every misclassification that would map to the \texttt{other} group is bold.}
      \label{table:confusionMisclassification}
   \end{center}
\end{table}

\subsection{Compressed Results}
\label{class-evaluation-compressed-results}
Table~\ref{table:classification-summary-compressed} shows a summary of the compressed results.
After the classes have been compressed, the SMO classifier is still on top but now with a best accuracy
of \textbf{.8583}. Compressing the classes into groups greatly increases the accuracy of the classifiers.

It is important to note that not only is overall accuracy important, but also the precision for the \texttt{snafu} group.
If SPOONS is stable capturing and classifying anomalous traffic, then when there is a spike then it will be visible.
However if there are too many false positives, then a spike cannot be trusted. Recall affects the size of a spike, but precision
affects the shape of the spike.

Table~\ref{table:confusionPrecisionRecall} shows an example of a compressed classification confusion matrix.
The \textbf{bold} cells shows \texttt{snafu} tweets that were misclassified as other groups. Our number one priority is to minimize these cells.
These cells represent the precision of the classifier for the \texttt{snafu} group (lower numbers generate higher precision).
The \emph{italics} cells shows tweets that were incorrectly classified as \texttt{snafu}. Although we also want to minimize these
cells, they are not as important as the \textbf{bold} cells.

\begin{table}[H]
   \begin{center}
      \scalebox{0.62}{
         \begin{tabular}{|c|c|c|c|}
         \hline
            \diaghead{\theadfont Diag ColumnmnHead II}{Classified}{Actual} & media & snafu & other \\
         \hline
            media & 95 & \textbf{0} & 8 \\
         \hline
            snafu & \emph{1} & 252& \emph{51} \\
         \hline
            other & 13 & \textbf{77} & 336\\
         \hline
         \end{tabular}
      }
      \caption[Misclassified Snafu]{A sample compressed classification confusion matrix showing misclassified \texttt{snafu} tweets.}
      \label{table:confusionPrecisionRecall}
   \end{center}
\end{table}

Full results for both the uncompressed and compressed evaluation can be found in Appendix~\ref{appendix-full-classifier}.

\input{classifierSummaries.tex}

\chapter{Outage Detection}
\label{outage-detection}

\section{Ground Truth}
\label{outage-detection-truth}
Netflix has provided us with a list of outages that occurred between March 14, 2011 and January 30, 2012.
This list is not comprehensive and some of the times are questionable. Some of the outages contained in the list
are internal outages that did not affect their streaming service. These outages generated no signal on Twitter.
Therefore, errors of omission could fall into one of two categories: true failures to recognize outages, and uncatchable
outages. Regardless, we use this as our ground truth about all of the Netflix outages in that time period.

\section{Success Metrics}
\label{outage-detection-metrics}
Recall from Section~\ref{background-accuracy}, the definition of \textbf{True Positive}, \textbf{False Positive}, and \textbf{False Negative}.
In the context of outage detection, these metrics have more specific definitions:
\begin{itemize}
   \item True Positive --- Any intersection between a reported outage range and a detected outage range.
   \item False Positive --- Any detected outage that has no intersection in the events reported by Netflix.
   \item False Negative --- An alert that has no intersection on an event reported by Netflix is a false negative.
\end{itemize}

\paragraph{Precision.}
Netlfix has specified an acceptable precision of 0.5.
This means that SPOONS is allowed to generate an incorrect alert for ever correct alert.
Any precision lower than 0.5 will cause SPOONS to be too noisy and consume too much engineer time.

% Netflix Coverage: 0.334129995406
\paragraph{Coverage.}
The list of outages that Netflix supplied has a coverage of 0.3341.
SPOONS tries to minimize the coverage to supply directed outages with low noise.

\subsection{Adjusted Score}
\label{outage-detection-metrics-adjusted-score}
Because of the coverage problem, outage detection methods cannot be compared using just their F$_{0.5}$ scores.
Therefore, they will be compared using an adjusted score that rewards a high F$_{0.5}$ score and punishes a high coverage.

\begin{equation}
   Adjusted Score = F_{0.5} \times (1 - coverage)
\end{equation}

\section{Monitors}
\label{outage-detection-monitors}
SPOONS uses a number of Monitors that use different methods to try and detect anomalies in the traffic.
This section describes each Monitor used.
Each Monitor described has two tables describing the parameters that can tune this Monitor and
the inputs that the Monitor takes. In addition, each Monitor will have a text description along with it.

\subsection{Baseline Monitor}
\label{outage-detection-monitors-Baseline}
\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Parameter & Description & Restrictions \\
         \hline
            B & Baseline & $ B > 0 $\\
         \hline
      \end{tabular}
   \end{center}
\end{table}

\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Input & Description & Restrictions \\
         \hline
            X & Any time series & - \\
         \hline
      \end{tabular}
   \end{center}
\end{table}

The Baseline Monitor looks for any value above B, and counts that as anomalous.
Ironically, because of its na\"{i}vet\'{e} it also provides a decent baseline for the Monitors.

For example, the Baseline Monitor can be used to monitor the number of tweets classified as \texttt{bad}.
The Monitor can be assigned a baseline of 200.
Therefore whenever there are more than 200 \texttt{bad} tweets in a period, the Baseline Monitor will alert.

\subsection{Windowed Standard Deviation Monitor}
\label{outage-detection-monitors-WindowStdDev}
\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Parameter & Description & Restrictions \\
         \hline
            W & Window Size & $ W > 0 $\\
         \hline
            L & Lower Threshold & $L > 0 $\\
         \hline
            U & Upper Threshold & $U >= L $\\
         \hline
      \end{tabular}
   \end{center}
\end{table}

\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Input & Description & Restrictions \\
         \hline
            X & Any time series & - \\
         \hline
      \end{tabular}
   \end{center}
\end{table}

The Windowed Standard Deviation Monitor is one of the simplest Monitors. This Monitor takes a $W$ sized window worth of data
and uses the standard deviation of the window to find outliers. Any outliers more than $L$ standard deviations away are considered anomalies and counts towards
an alert, but are still included in the windowed standard deviation calculation. Any value more than $U$ standard deviations away is considered an anomaly, but not included
in the standard deviation calculation. The reason for this is that values above $U$ are extreme outliers.

The calculation for the standard deviation is based off of an iterative approach described in Knuth's ``The Art of Computer Programming''\cite{Knuth}.
Because Knuth's approach is iterative, it can be modified it to calculate for a range of values in an on-line fashion.

Adding the $k$th value, $x$, to the window:
$$
   Mean(k) = \frac{Mean(k - 1) * (k - 1) - Mean(k - W) + x}{k}
$$
$$
   V(k) = (x - Mean(k)) * (x - Mean(k - 1))
$$
$$
   T(k) = T(k - 1) - V(k - W) + V(k)
$$
$$
   WindowStdDev(k) = \sqrt{\frac{T(K))}{k - 1}}
$$

For example, the Windowed Standard Deviation Monitor can be used to monitor the number of tweets classified as \texttt{bad} with a lower threshold of 2, an upper threshold of 3, and a window size of 3.
Table~\ref{table:weeklyWindowEx} shows the process of the Windowed Standard Deviation Monitor at each iteration.
Note that iterations 1 - 3 are filling the window and will never alert.
Iteration 5 observed a value that was over the lower threshold, so generated an alert. However it as under the upper threshold, so was included in the window.
Iteration 6 observed a value that was greater than the upper threshold. Therefore, that value produced an alert and was not included in the window.

\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|c|c|c|}
         \hline
            Iteration & Window & Mean & Standard Deviation & Observed Point & Alert?
         \tabularnewline\hline
            1 & - & - & - & 5 & No
         \tabularnewline\hline
            2 & 5 & 5 & 0 & 6 & No
         \tabularnewline\hline
            3 & 5, 6 & 5.5 & 0.7071 & 3 & No
         \tabularnewline\hline
            4 & 5, 6, 2 & 4.3333 & 2.0817 & 6 & No
         \tabularnewline\hline
            5 & 6, 2, 6 & 4.6667 & 2.3094 & 10 & Yes
         \tabularnewline\hline
            6 & 2, 6, 10 & 6 & 4 & 20 & Yes
         \tabularnewline\hline
            6 & 2, 6, 10 & 6 & 4 & 6 & No
         \tabularnewline\hline
      \end{tabular}
   \end{center}
   \caption[Windowed Standard Deviation Monitor Example]{An example of how the Windowed Standard Deviation Monitor determines when to alert.}
   \label{table:weeklyWindowEx}
\end{table}

\subsection{Weekly Offset Windowed Standard Deviation Monitor}
\label{outage-detection-monitors-WeeklyWindowStdDev}
\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Parameter & Description & Restrictions \\
         \hline
            W & Window Size & $ W > 0 $\\
         \hline
            L & Lower Threshold & $L > 0 $\\
         \hline
            U & Upper Threshold & $U >= L $\\
         \hline
      \end{tabular}
   \end{center}
\end{table}

\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Input & Description & Restrictions \\
         \hline
            X & Any time series & - \\
         \hline
      \end{tabular}
   \end{center}
\end{table}

The Weekly Offset Window Standard Deviation Monitor leverages the periodicity of tweet volume.
Not only is there a daily pattern in traffic, but there is also an even stronger weekly pattern.
The stronger weekly pattern makes sense if one views Netflix-related Twitter traffic as a representative for the number of
people currently watching Netflix. People tend to have pattern that they follow, and people are more available
on different days of the week (especially Friday).

This Monitor holds a windowed standard deviation for every 30 minute time period with 15 minute offsets
for every week. Therefore, values are not compared to the other values around it, but to expected values from
previous weeks. This Monitor uses the same tactics as the Windows Standard Deviation Monitor for counting anomalies.

For example, the Weekly Offset Windowed Standard Deviation Monitor can be used to monitor the number of tweets classified as \texttt{bad}.
Unlike the Baseline Monitor, this Monitor allows for both weekly trends and daily variance.
Therefore a period of 200 \texttt{bad} tweets may be acceptable on a Friday evening when traffic is typically high, but considered anomalous on a Tuesday morning when traffic is typically low.

\subsection{Mean Squared Error Monitor}
\label{outage-detection-monitors-MSE}
\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Parameter & Description & Restrictions \\
         \hline
            W & Window Size & $ W > 0 $\\
         \hline
            T & Threshold & $ T > 0 $\\
         \hline
      \end{tabular}
   \end{center}
\end{table}

\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Input & Description & Restrictions \\
         \hline
            X & The expected time series & - \\
         \hline
            Y & The actual time series & $ Y(k) \leq X(k) $ \\
         \hline
      \end{tabular}
   \end{center}
\end{table}

The Mean Square Error Monitor keeps a windowed mean squared error (MSE). This Monitor requires two sets of input, a set
of expected values and a set of actual values. Any value that causes the MSE to go above a certain threshold, $T$,
counts towards an anomaly.

Adding the $k$th value to a full window:
$$
   V(k) = (X(k) - Y(k))^{2}
$$
$$
   MSE(k) = \frac{MSE(k - 1) * W - V(k - W) + V(k)}{W}
$$

For example, the Mean Square Error Monitor can be used to monitor the number of tweets classified as \texttt{bad} by letting $X$ be the actual volume of tweets and $Y$ be the number of total tweets \textbf{not} classified as \texttt{bad}.
Table~\ref{table:mseEx} shows the process of a Mean Square Error Monitor with a window size of 3 and a threshold of 100 at each iteration.
Iteration 5 produces an alert because an MSE over the threshold was observed.
An MSE over the threshold means that the number of tweets excluding tweets classified as \texttt{bad} became a poor predictor for the actual traffic of all tweets.
This indicates that the traffic of \texttt{bad} tweets is becoming a significant factor to the overall Netflix-related Twitter traffic, which signifies an outage.

\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|c|c|c|c|}
         \hline
            Iteration & Window (Errors) & Old MSE & Observed X & Observed Y & New MSE & Alert?
         \tabularnewline\hline
            1 & - & - & 100 & 95 & 25 & No
         \tabularnewline\hline
            2 & 5 & 25 & 105 & 95 & 62.5 & No
         \tabularnewline\hline
            3 & 5, 10 & 62.5 & 100 & 98 & 43 & No
         \tabularnewline\hline
            4 & 5, 10, 2 & 43 & 95 & 88 & 51 & No
         \tabularnewline\hline
            5 & 10, 2, 7 & 51 & 100 & 80 & 151 & Yes
         \tabularnewline\hline
      \end{tabular}
   \end{center}
   \caption[MSE Monitor Example]{An example of how the MSE Monitor determines when to alert.}
   \label{table:mseEx}
\end{table}

\subsection{Ratio Monitor}
\label{outage-detection-monitors-Ratio}
\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Parameter & Description & Restrictions \\
         \hline
            T & Threshold & $ 0 < T < 1 $\\
         \hline
      \end{tabular}
   \end{center}
\end{table}

\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Input & Description & Restrictions \\
         \hline
            X & The expected time series & - \\
         \hline
            Y & The actual time series & $ Y(k) < X(k) $\\
         \hline
      \end{tabular}
   \end{center}
\end{table}

The Ratio Monitor takes the ratio of the actual value over the expected value for every time period.
Whenever the ratio dips under the threshold $T$, then that period counts towards an anomaly.
This Monitor may seem simple, but the real challenge lies in picking a proper $X$ and $Y$.
If a good approximating time series can be chosen, then the Monitor can be very successful.

For example, the Ratio Monitor can be used with $X$ being the actual volume of tweets and $Y$ being the number of total tweets \textbf{not} classified as \texttt{bad}.
Therefore, any divergence in $X$ and $Y$ would be caused by the traffic of tweets classified as \texttt{bad}.

\subsection{Class Correlation Monitor}
\label{outage-detection-monitors-Correlation}
\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Parameter & Description & Restrictions \\
         \hline
            W & Window Size & $ W > 0 $\\
         \hline
            T & Threshold & $ -1 < T < 1 $\\
         \hline
      \end{tabular}
   \end{center}
\end{table}

\begin{table}[H]
   \begin{center}
      \begin{tabular}{|c|c|c|}
         \hline
            Input & Description & Restrictions \\
         \hline
            X & The expected time series & - \\
         \hline
            Y & The actual time series & $ Y(k) < X(k) $\\
         \hline
      \end{tabular}
   \end{center}
\end{table}

The Correlation Monitor takes the Pearson Correlation between $X$ and $Y$ for a running window of size $W$.
Pearson Correlation is used because of its ability to catch the linear correlation between two time series within
a normalized range.

For performance reasons SPOONS uses an approximation of Pearson Correlation which uses the windowed standard deviation approach
described in Section~\ref{outage-detection-monitors-WindowStdDev}.

Adding the $k$th value, to the window:
\begin{center}
   Let $\bar{X}$ be the windowed mean of $X$.
\end{center}
\begin{center}
   Let $\bar{Y}$ be the windowed mean of $Y$.
\end{center}
\begin{eqnarray*}
   \lefteqn{T(k) = }\\
   && T(k - 1) + (X(k) * Y(k)) - (X(k - W) * Y(k -W))
\end{eqnarray*}
\begin{eqnarray*}
   \lefteqn{Pearson(k) = }\\
   && \frac{T(k) - (W * \bar{X} * \bar{Y})}{(W - 1) * WindowStdDev(X) * WindowStdDev(Y)}
\end{eqnarray*}


For example, the Correlation Monitor can use the same tactic described by the MSE and Ratio Monitors,
By letting $X$ be the actual volume of tweets and $Y$ be the number of total tweets \textbf{not} classified as \texttt{bad}, the Correlation Monitor can view the effects of tweets classified as \texttt{bad}.

\section{Evaluation}
\label{outage-detection-evaluation}
The classifiers chosen to be used in the outage detection evaluation are the top ten classifiers from the classifier evaluation (see Section~\ref{class-evaluation-results}).
Each of the ten classifiers are paired with one of the monitors six monitors discussed in Section~\ref{outage-detection-monitors} making 60 different combinations as shown in Table~\ref{table:outageDetectionEvaluationSpace}.

\begin{table}[H]
   \footnotesize
   \begin{center}
      \begin{tabular}{|l|l|}
         \hline
            \textbf{Classifiers} & \textbf{Filtering}
         \tabularnewline\hline
            SMO & Full
         \tabularnewline\hline
            SMO & None
         \tabularnewline\hline
            BPNB & Full
         \tabularnewline\hline
            BinarySMO & None
         \tabularnewline\hline
            BinarySMO & None
         \tabularnewline\hline
            BinaryNaiveBayes & None
         \tabularnewline\hline
            BinaryNaiveBayes & Full
         \tabularnewline\hline
            NaiveBayes & None
         \tabularnewline\hline
            BPNB & None
         \tabularnewline\hline
            J48 & Full
         \tabularnewline\hline
      \end{tabular}
      {
         \Huge
         $\times$
      }
      \begin{tabular}{|l|}
         \hline
            \textbf{Monitor}
         \tabularnewline\hline
            Baseline
         \tabularnewline\hline
            Windowed Standard Deviation
         \tabularnewline\hline
            Weekly Offset Windowed Standard Deviation
         \tabularnewline\hline
            Mean Squared Error
         \tabularnewline\hline
            Ratio
         \tabularnewline\hline
            Class Correlation
         \tabularnewline\hline
      \end{tabular}
      \caption[Outage Detection Evaluation Combinations]{The cross product of the classifier and Monitor will be used to evaluate the outage detection methods.}
      \label{table:outageDetectionEvaluationSpace}
   \end{center}
\end{table}

After the combination is chosen, the classifier will be fun on all of the tweets in the evaluation period.
The Monitor will then use the classified data to auto-tune (Section~\ref{arch-autotuning}).
The output will be a set of optimal parameters and a confusion matrix with the actual results.
Table~\ref{outage-detection-results-example} shows an example of the J48 classifier paired with the Weekly Window Standard Deviation Monitor.
Note that the confusion matrix is much more simple than the classification confusion matrix.
The ``True'' label indicates an outage.
It does not make sense to fill the true negative (false/false) cell because the confusion matrix is dealing in outages and that cell represents the number of non-outages that were correctly classified as non-outages.
Since neither precision nor recall uses this cell, it can be left undefined.

\begin{table}[H]
   \begin{center}
      \footnotesize
      \begin{tabular}{|l|c|}
         \hline
            Parameter & Value
         \tabularnewline\hline
            Resistance Method & Fighting
         \tabularnewline\hline
            Alert Resistance & 10
         \tabularnewline\hline
            Recovery Resistance & 6
         \tabularnewline\hline
            Smoothing Method & None
         \tabularnewline\hline
            Lower Tolerance & 1.0
         \tabularnewline\hline
            Upper Tolerance & 6.0
         \tabularnewline\hline
            Window Size & 10
         \tabularnewline\hline
      \end{tabular}
      \begin{tabular}{|c|c|c|}
         \hline
            \diaghead{\theadfont ABCDEFGHIJKL}{Predicted}{Actual} & True & False
         \tabularnewline\hline
            True & 84 & 4
         \tabularnewline\hline
            False & 120 & X
         \tabularnewline\hline
      \end{tabular}
      \caption[Example Outage Detection Results]{An example of the output for an outage detection method.}
      \label{outage-detection-results-example}
   \end{center}
\end{table}

\section{Results}
\label{outage-detection-results}
Table~\ref{table:outage-detection-summary} shows a summary of the outage detection evaluation results.
The SMO classifier took the top place when paired with the Weekly Window Standard Deviation Monitor.
The full outage detection results are in Appendix~\ref{appendix-full-outage-detection}.

\input{outageSummaries.tex}

\part{Conclusions}
\label{conclusions}

\chapter{Conclusions}
\label{conclusions-conclusions}
Section~\ref{class-evaluation-results} shows that it is possible to use classifiers to accurately classify Netflix-related tweets.
Section~\ref{outage-detection-results} shows that it is possible to use those classified tweets to detect Netflix service outages.

\chapter{Current Limitations of SPOONS}
\label{limitations}
\paragraph{Real Time Tuning.}
All the results generated were the result of auto-tuning on the entire set of data.
This assumes that the traffic that occurs in the future will be similar to the traffic that occurred in the past.
Ideally, SPOONS would continually re-tune itself every time new outage data is acquired.

\paragraph{Severity.}
SPOONS does not try to determine how severe an outage it.
Severity can be measures in two different ways: breadth and depth.
The breath measure would be how many different platforms or regions are affected.
Both of these pieces of information could be available through tweets.
The depth measure would be how many users are affected.
This measure is much more simple, it would be relative to the size of the outage spike.

\paragraph{Malicious Tweet Attack.}
SPOONS is currently susceptible to a type of attack where a user generates many fake tweets.
SPOONS currently makes no attempt to verify the validity of a tweet, or the credibility of
its author. Therefore, an attacker can just generate hundreds of simple tweets like:
\begin{center}
   \texttt{Netflix is down.}
\end{center}
This will be probably be enough to cause SPOONS to report an outage when there is none.

\paragraph{Nature of an Outage.}
Currently, SPOONS cannot detect the root cause of an outage.
Even though during an outage there are many tweets like:
\begin{center}
   \texttt{Damn NetFlix via Xbox 360 DOWN! (Bbm Sad Face)}
\end{center}
SPOONS does not yet try to determine the cause of an outage.

\chapter{Current and Future Work}
\label{future-work}

\section{WEKA Classifier Reimplementation}
\label{future-work-weka}
The WEKA machine learning package offers a wide variety of classifiers.
However, their implementation and API has some room for improvement.
Because of this, SPOONS already uses two classifiers implemented from scratch.
I plan on continuing this to make a classification package centered around performance
and ease of use.

\section{Advanced Sentiment Analysis}
\label{future-work-kim}
Kim Paterson, a member of the SPOONS team, is currently working on improving the sentiment
analysis work from Cailin Cushing\cite{cailinThesis}. If completed, then SPOONS can use
both text classification and sentiment analysis to determine when there is an outage.
Because of their orthogonal natures, having both would allow SPOONS to recognize even more outages.

\section{SPOONS Scaling}
\label{future-work-brett}
Another member of the SPOONS team, Brett Armstrong, is working to improve the scalability of SPOONS.
Because of its distributed architecture (see Chapter~\ref{arch-dist}), SPOONS already has the potential to
scale horizontally. If there is too much traffic/work, then another server can just be added to the cluster.
However, that currently requires manual intervention. Since there are spikes when outages occur, we may not know
when there is going to be a lot of traffic. To solve this problem, Brett will use SPOONS to monitor itself.
The end result of an Analysis Pipeline will not be an email alert, rather it will be the creation of a new
AWS instance.

% ------------- End main chapters ----------------------

\clearpage
\bibliography{bibliography}
\bibliographystyle{plain}
%\addcontentsline{toc}{chapter}{Bibliography}

\appendix
\chapter{SPOONS Database Schema Highlights}
\label{appendix-db-schema}

% TODO(eriq): Pick more tables

\section{DATA\_tweets}
\begin{lstlisting}
CREATE TABLE DATA_tweets (
   twitter_id varchar(32) COLLATE utf8_unicode_ci NOT NULL,
   published int(11) NOT NULL,
   content text COLLATE utf8_unicode_ci NOT NULL,
   source text COLLATE utf8_unicode_ci,
   lang varchar(3) COLLATE utf8_unicode_ci NOT NULL,
   author varchar(50) COLLATE utf8_unicode_ci NOT NULL DEFAULT 'Jon Doe',
   frame_id int(11) DEFAULT NULL,
   id int(11) NOT NULL AUTO_INCREMENT,
   place text COLLATE utf8_unicode_ci,
   geo text COLLATE utf8_unicode_ci,
   PRIMARY KEY (id),
   UNIQUE KEY tweet_id (twitter_id),
   UNIQUE KEY twitter_id (twitter_id),
   KEY frame_index (frame_id),
   KEY published_index (published),
   KEY lang (lang)
);
\end{lstlisting}

\chapter{Full Classifier Evaluation Results}
\label{appendix-full-classifier}
\input{classifierTables.tex}
\clearpage

\chapter{Full Outage Detection Evaluation Results}
\label{appendix-full-outage-detection}
\input{outageResults.tex}
\clearpage

\chapter{Full Training Set}
\label{appendix-full-training}
\input{trainingSet.tex}
\clearpage

% Glossary here plz
\newglossaryentry{SPOONS}{name={SPOONS},
                          description={Swift Perception Of Online Negative Situations. The name
                                       of the system presented in this paper}}

\newglossaryentry{Twitter}{name={Twitter},
                           description={Twitter is a social media service that allows users to post
                                        tweets (micro-posts) about any topic}}

\newglossaryentry{Tweet}{name={Tweet},
                         description={A micro-post to a Twitter service. Tweets are limited to 140
                                      characters}}

\newglossaryentry{Netflix}{name={Netflix},
                           description={Inc. [NASDAQ: NFLX] is the world's leading Internet
                                        subscription service for enjoying movies and TV series
                                        with more than 23 million streaming members in the United
                                        States, Canada, Latin America, the United Kingdom
                                        and Ireland\cite{netflix}}}

\newglossaryentry{Time Series Analysis}
      {name={Time Series Analysis},
       description={The analysis of a series of data points over time. In this work those data
                    points are the volume or estimated sentiment of a subset of the traffic about
                    Netflix on Twitter during a time period}}

\newglossaryentry{Real Time}
      {name={Real Time},
       description={Some of Netflix's services stream to customers in real time which means the
                    users expect to get immediate responses from those services. So when
                    they go down, the customers want the problem to be fixed immediately. These
                    analysis methods need to have real time responses that are as close to
                    immediate detection as possible. This means that the system needs to use
                    whatever information it has available to it up to right before the outage to
                    detect the event and alert Netflix engineers}}

\newglossaryentry{aws}
      {name={AWS},
       description={Amazon Web Services. Cloud computing offered by Amazon}}

\newglossaryentry{ec2}
      {name={EC2},
       description={Elastic Compute Cloud. Instance based cloud computing machines offered through AWS}}

% Don't really care for glossary
%\glsaddall
%\addcontentsline{toc}{chapter}{Glossary}
%\printglossaries

\end{document}
